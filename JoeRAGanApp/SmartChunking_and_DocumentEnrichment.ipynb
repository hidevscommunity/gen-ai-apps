{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Transcripts into individual LangChain Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import JSONLoader, DirectoryLoader\n",
    "import jq\n",
    "# Define the metadata extraction function.\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "\n",
    "    metadata[\"video_id\"] = record.get('video_id')\n",
    "    metadata[\"episode_title\"] = record.get(\"Episode Title\")\n",
    "    metadata[\"guest\"] = record.get(\"Guest\")\n",
    "    metadata[\"video_url\"] = record.get(\"URL\")\n",
    "    metadata[\"date_posted\"] = record.get(\"Date\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    'Transcripts/', \n",
    "    glob='*.json',\n",
    "    loader_cls=JSONLoader,\n",
    "    loader_kwargs={'jq_schema' : '.[]', 'content_key' : \"text\", 'metadata_func' : metadata_func},\n",
    "    show_progress = True   \n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Naive Text Splitter to split at 1000 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken \n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "#Create function to check token length\n",
    "def tiktoken_len(text):\n",
    "    tokens= tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special = ()\n",
    "    )\n",
    "    return len(tokens)\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 1000, chunk_overlap = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the System Message for GPT-4 to perform the smart chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_message(text_type, text_title):\n",
    "    system_message = f'''Given some text, which is part of a {text_type} from {text_title}, your goal is to split the text in half so that no thought or topic is cutoff and the split is performed at the end of a complete topic.\n",
    "\n",
    "        You will be given steps to follow until the final desired result is achieved. \n",
    "\n",
    "        Some important things to note:\n",
    "        - '!MIDPOINT!' denotes the midpoint of the text.\n",
    "        - To complete this task effectively you must adhere to strictly to the instructions at each step\n",
    "        - You must be exact with your output, whenever providing words from the text, copy exactly what is written, even if there are missing words, repeated words or misspellings, it does not matter.\n",
    "\n",
    "        Step 1 - Determine what the main topics are before and after the !MIDPOINT! label. Main topics are overall topics to which the text is about, not brief things that are mentioned in passing. Create a list of topics as such:\n",
    "        Before Midpoint = Topic 1, Topic 2, Topic 3\n",
    "        After Midpoint = Topic 4, Topic 5, Topic 6\n",
    "\n",
    "        Step 2 - Based on the the lists of Topic Labels you have created identify if either of these conditions are true. \n",
    "        - The last topic of the first section is semantically related to the first topic of the second section\n",
    "        - The last topic of the first section continues on past the !MIDPOINT!. \n",
    "\n",
    "        Step 3 - Depending on the condition that you have identified from Step 2, decide which of the following course of actions must be taken:\n",
    "        - If there was semantic overlap between the two topics then the words you split on should be located at the conclusion of overlapping topics. When the transition to the next semantically unrelated topic begins.\n",
    "        - If there is a continuation of the last topic of the first section past the !MIDPOINT! then the text needs to be split where this continuation ends.\n",
    "        - If neither of these conditions where meet then you must check if the current location of the !MIDPOINT! interrupts the completion of a topic, if this is the case then the split point should be where the interrupted topic concludes. \n",
    "        \n",
    "        \n",
    "        ​​Step 4- Based on the course of action you have identified in Step 3, perform this course of action and locate a small set of exact words on which to split the text. The words must be exact and should not be long. If there appears to be puncutation present in the text, the location to split should always be after the completion of a sentence.\n",
    "\n",
    "        Step 5 - Given the exact words on where to split reorganize the topics so that they match the new sections which are determined by the split location.\n",
    "        For example if in the example from Step 1, Topic 3 and Topic 4 have overlap then the lists would now be as follows:\n",
    "        Before Split = Topic 1, Topic 2, Topic 3, Topic 4\n",
    "        After Split = Topic 5, Topic 6\n",
    "\n",
    "        Final Step - Now that we know where the text should be split and the new organization of topics, provide the final output which is a python dictionary with 3 key-value pairs:\n",
    "        - \"before_split_topics\" will be the 'Before Split' list that you identified in Step 3.\n",
    "        -  \"after_split_topics\" will be the 'After Split' list that you identified in Step 3.\n",
    "        - \"split_key\" will be the exact words that identify where the text should be split.\n",
    "        An example of what the final output should look like structurally:\n",
    "\n",
    "        {{\"before_split_topics : [\"Topic 1\", \"Topic 2\", \"Topic 3\", \"Topic 4\",], \"after_split_topics\" : [\"Topic 5\", \"Topic 6\"],\"split_key\" : \"split the text here\"}}\n",
    "\n",
    "        Begin!'''\n",
    "    \n",
    "    return system_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the User Message for GPT-4 to perform the smart chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "def create_user_message(bigtext):\n",
    "    user_template = PromptTemplate.from_template(\"TEXT: \\n  {bigtext} \\n Remember follow the outlined 6 step plan. The location you decide to split on should be near the !MIDPOINT!. If there appears to be puncutation present in the text, the location to split should always be after the completion of a sentence. Write the out the result of each step and then the final output: \")\n",
    "    return user_template.format(bigtext = bigtext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function that organizes the logic behind the smart chunking and document enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import time\n",
    "import ast\n",
    "\n",
    "#Takes in the transcript as a singular LangChain Document.\n",
    "def smart_chunking(doc):\n",
    "    #First we perform a naive split on the LangChain Doc using a token length of 1000.\n",
    "    documents = splitter.split_documents([doc])\n",
    "\n",
    "    #Get the Title of the Text so the LLM has some context\n",
    "    text_title = documents[0].metadata[\"episode_title\"]\n",
    "\n",
    "    #Give the Type of Text for context\n",
    "    text_type = \"Transcript\" #User should edit it this\n",
    "    chat = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "    #Use this for the two chunks\n",
    "    final_docs = {}\n",
    "    changed = []\n",
    "    dicts_of_changes = []\n",
    "\n",
    "    for i, chunk in enumerate(documents):\n",
    "        print(f\"Index = {i}\")\n",
    "        if i + 1 < len(documents):\n",
    "            first = documents[i].page_content\n",
    "            second = documents[i+1].page_content\n",
    "            full_text = first + ' !MIDPOINT! ' + second\n",
    "            #Create the individualized User and System Message\n",
    "            hum_message = create_user_message(full_text)\n",
    "            sys_message = create_system_message(text_title=text_title,text_type=text_type)\n",
    "\n",
    "            messages = [\n",
    "                SystemMessage(content = sys_message),\n",
    "                HumanMessage(content = hum_message)\n",
    "                ]\n",
    "            response = chat(messages)\n",
    "            output = response.content\n",
    "\n",
    "            print(output)\n",
    "\n",
    "            #Process to extract the dictionary\n",
    "            \n",
    "            #Extract index of brackets\n",
    "            open_bracket_index = output.find('{')\n",
    "            closed_bracket_index = output.find('}') + 1\n",
    "\n",
    "            #Slice those indicies\n",
    "            dictionary_string = output[open_bracket_index:closed_bracket_index]\n",
    "\n",
    "            #Convert the string to dictionary literal\n",
    "            boundary_dict = ast.literal_eval(dictionary_string)\n",
    "            dicts_of_changes.append(boundary_dict)\n",
    "\n",
    "            total_text = first + ' ' + second\n",
    "\n",
    "            end_index_first_chunk = total_text.find(boundary_dict['split_key'])\n",
    "            print(end_index_first_chunk)\n",
    "\n",
    "            new_first_chunk = total_text[0:end_index_first_chunk]\n",
    "            new_second_chunk = total_text[end_index_first_chunk:]\n",
    "\n",
    "            changed.append(new_first_chunk)\n",
    "            changed.append(new_second_chunk)\n",
    "\n",
    "            #ADD TO THE DICT THAT KEEPS TRACK OF THE DOCS\n",
    "            final_docs[f'Chunk {i}'] = {\n",
    "                'Text' : new_first_chunk, 'Topics' : boundary_dict['before_split_topics']\n",
    "            }\n",
    "            final_docs[f'Chunk {i + 1}'] = {\n",
    "                'Text' : new_second_chunk, 'Topics' : boundary_dict['after_split_topics']\n",
    "            }\n",
    "\n",
    "            print(f\"Old first: \\n {first} \\n New first: \\n {new_first_chunk} \\n Old Second: \\n {second} \\n New Second: \\n {new_second_chunk}\")\n",
    "\n",
    "            documents[i].page_content = new_first_chunk\n",
    "            documents[i].metadata[\"Topics\"] = boundary_dict['before_split_topics']\n",
    "            documents[i+1].page_content = new_second_chunk\n",
    "            documents[i+1].metadata[\"Topics\"] = boundary_dict['after_split_topics']\n",
    "            time.sleep(1)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neil_chunks = smart_chunking(docs[0])\n",
    "final_doc_list = []\n",
    "for chunk in neil_chunks:\n",
    "    temp = {'text' : chunk.page_content}\n",
    "    del chunk.metadata['source']\n",
    "    temp.update(chunk.metadata)\n",
    "    final_doc_list.append(temp)\n",
    "\n",
    "import json\n",
    "with open(f'LangChain Documents/episode_1904.json','w') as f:\n",
    "    f.write(json.dumps(final_doc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_chunks = smart_chunking(docs[2])\n",
    "final_doc_list = []\n",
    "for chunk in mark_chunks:\n",
    "    temp = {'text' : chunk.page_content}\n",
    "    del chunk.metadata['source']\n",
    "    temp.update(chunk.metadata)\n",
    "    final_doc_list.append(temp)\n",
    "\n",
    "import json\n",
    "with open(f'LangChain Documents/episode_1863.json','w') as f:\n",
    "    f.write(json.dumps(final_doc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elon_chunks = smart_chunking(docs[1])\n",
    "final_doc_list = []\n",
    "for chunk in elon_chunks:\n",
    "    temp = {'text' : chunk.page_content}\n",
    "    del chunk.metadata['source']\n",
    "    temp.update(chunk.metadata)\n",
    "    final_doc_list.append(temp)\n",
    "\n",
    "import json\n",
    "with open(f'LangChain Documents/episode_1470.json','w') as f:\n",
    "    f.write(json.dumps(final_doc_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
