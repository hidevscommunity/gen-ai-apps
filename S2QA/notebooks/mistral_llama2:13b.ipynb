{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e33dced-e587-4397-81b3-d6606aa1738a",
   "metadata": {},
   "source": [
    "# S2QA: Mistral vs. Llama2:13B\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5863dde9-84a0-4c33-ad52-cc767442f63f",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, follow the [readme](https://github.com/jmorganca/ollama#building) to set up and run a local Ollama instance.\n",
    "\n",
    "This demo is running -\n",
    "\n",
    "```bash\n",
    "./ollama run llama2:13b-chat\n",
    "```\n",
    "\n",
    "```bash\n",
    "./ollama run mistral\n",
    "```\n",
    "\n",
    "When the Ollama app is running on your local machine:\n",
    "\n",
    "- All of your local models are automatically served on localhost:11434\n",
    "- Select your model when setting llm = Ollama(..., model=\"<model family>:<version>\")\n",
    "- If you set llm = Ollama(..., model=\"<model family\") without a version it will simply look for latest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad297f19-998f-4485-aa2f-d67020058b7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T02:07:09.825521Z",
     "start_time": "2023-09-12T02:07:08.193329Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.query_engine import CitationQueryEngine\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    ServiceContext,\n",
    ")\n",
    "from llama_index.response.notebook_utils import display_response\n",
    "from llama_hub.semanticscholar.base import SemanticScholarReader\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "152ced37-9a42-47be-9a39-4218521f5e72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-12T02:07:10.344425Z",
     "start_time": "2023-09-12T02:07:10.340295Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_answer_from_model(model, query_space, query_string):\n",
    "    embed_model = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=model)\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        llm=Ollama(model=model), embed_model=embed_model\n",
    "    )\n",
    "    s2reader = SemanticScholarReader()\n",
    "\n",
    "    # increase limit to get more documents\n",
    "    documents = s2reader.load_data(query=query_space, limit=10)\n",
    "\n",
    "    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "    query_engine = CitationQueryEngine.from_args(\n",
    "        index,\n",
    "        similarity_top_k=5,\n",
    "        citation_chunk_size=512,\n",
    "    )\n",
    "    # query the citation query engine\n",
    "    response = query_engine.query(query_string)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd12f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_space = \"biases in large language models\"\n",
    "query_string = \"explain all the biases in large language models in a markdown table\"\n",
    "\n",
    "response_mistral = get_answer_from_model(\"mistral\", query_space, query_string)\n",
    "response_llama = get_answer_from_model(\"llama2:13b\", query_space, query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f0d8d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** It is not possible to create a markdown table summarizing all the biases in large language models as the sources provided do not cover this topic exhaustively. However, I can provide you with an answer that highlights some of the potential biases discussed in the sources. \n",
       "\n",
       "Biases in large language models can stem from various factors such as the nature of training data, model specifications, algorithmic constraints, product design, and policy decisions. These biases can manifest in several ways, including:\n",
       "\n",
       "| Source | Description |\n",
       "| --- | --- |\n",
       "| Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT | Generative large language models like ChatGPT have been found to perform poorly at the segment level when used for assessing machine translation quality. However, by combining Chain-of-Thoughts and Error Analysis, a new prompting method called Error Analysis Prompting can improve LLMs' performance on this task. |\n",
       "| Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models | Large-scale language models like ChatGPT can learn biases from the data they have been trained on, potentially resulting in unintended consequences. Examples of such biases include those related to gender, race, and geography. The ethical concerns arising from these biases highlight the need for a multi-disciplinary, collaborative effort to develop more equitable, transparent, and responsible AI systems. |\n",
       "| Disclosing the Biases in Large Language Models via Reward Based Interrogation | Recent studies have shown that language models can learn to be biased towards certain genders. Fine-tuning these models on specific datasets can alleviate this problem for that dataset but does not resolve the actual issue. Changing the question asked to the language model can significantly alter the log probabilities of bias measured in responses and might lay the foundation for further alignment and safety problems in large language models. |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48dce5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Sure, I'd be happy to help! Here is the information you provided, formatted into a markdown table:\n",
       "\n",
       "| Source | Title | Citation Count | Open Access Pdf | Authors | External IDs |\n",
       "| --- | --- | --- | --- | --- | --- |\n",
       "| [1] | Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models | 41 | None | Emilio Ferrara | ArXiv, DBLP, DOI, CorpusId |\n",
       "| [2] | Biases in Large Language Models: Origins, Inventory, and Discussion | 4 | <https://dl.acm.org/doi/pdf/10.1145/3597307> | Roberto Navigli, Simone Conia, Bj√∂rn Ross | ACM Journal of Data and Information Quality, DBLP, DOI, CorpusId |\n",
       "| [3] | Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT | 22 | None | Qingyu Lu, Baopu Qiu, Liang Ding, Liping Xie, Dacheng Tao | arXiv, DBLP, DOI, CorpusId |\n",
       "\n",
       "Based on the information provided in these sources, here is a summary of the biases in large language models:\n",
       "\n",
       "| Type of Bias | Description | Source |\n",
       "| --- | --- | --- |\n",
       "| Data selection bias | Biases caused by the choice of texts that make up a training corpus. | [2] |\n",
       "| Social bias | Biases related to gender, age, religion, etc. present in the data used to train the model or in the way the model is designed. | [1], [2] |\n",
       "| Cultural bias | Biases related to cultural differences and nuances that are not captured by the training data. | [2] |\n",
       "| Lexical bias | Biases related to the choice of words and phrases used in the training data. | [2] |\n",
       "| Evaluation bias | Biases present in the evaluation metrics used to assess the performance of the model. | [1], [3] |\n",
       "| Contextual bias | Biases related to the context in which the model is being used, such as differences in language style or register. | [2] |\n",
       "| Model architecture bias | Biases related to the design and structure of the model itself, such as the choice of layers or activation functions. | [1], [3] |\n",
       "| Training data bias | Biases present in the training data, such as differences in the distribution of words or phrases. | [2] |\n",
       "\n",
       "I hope this helps! Let me know if you have any other questions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3f5dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_space = \"datasets on question answering\" \n",
    "query_string = \"list all the datasets on question answering in a markdown table\"\n",
    "\n",
    "response_mistral = get_answer_from_model(\"mistral\", query_space, query_string)\n",
    "response_llama = get_answer_from_model(\"llama2:13b\", query_space, query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "530f488f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Dataset | Description |\n",
       "| --- | --- |\n",
       "| Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets | This study explores three popular open-domain benchmark datasets to gain insight into their competencies. It finds that 30% of test-set questions have a near-duplicate paraphrase in the corresponding train sets, and 60-70% of answers are also present in the train sets. The study further shows that all models perform substantially worse on questions that cannot be memorized from the train sets, with a mean absolute performance difference of 61%. |\n",
       "| Video Question Answering: a Survey of Models and Datasets | This survey paper reviews various models and datasets used in video question answering. It highlights the importance of using contextual information from videos to answer questions accurately. The paper also discusses challenges faced by video question answering, such as dealing with varying lighting conditions, different camera angles, and language barriers. |\n",
       "| Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering | This study presents Science Question Answering (ScienceQA), a new benchmark that consists of multimodal multiple-choice questions with annotations of their answers. The authors designed language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. They found that CoT improves the question answering performance by 1.20% in few-shot GPT-3 and 3.99% in fine-tuned UnifiedQA. |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response_mistral.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab37b6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**`Final Response:`** Sure! Here is the information you requested, presented in a markdown table:\n",
       "\n",
       "| Dataset | Year | Paper ID | Citation Count | Open Access PDF | Authors | External IDs |\n",
       "| --- | --- | --- | --- | --- | --- | --- |\n",
       "| Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets | 2020 | cb58542c94ce83b09f5d3809e69518ba52709c92 | 143 | [1] | https://aclanthology.org/2021.eacl-main.86.pdf | Lewis, Sebastian, Pontus | DBLP, ArXiv, ACL, MAG, DOI, CorpusId |\n",
       "| A review of public datasets in question answering research | 2021 | 67ad6ce6e7fe48b850018a09750e791a7196f365 | 28 | [2] | None | Cambazoglu, Sanderson, Scholer, Croft | DBLP, DOI, CorpusId |\n",
       "| Video Question Answering: Datasets, Algorithms and Challenges | 2022 | 4cb65c1541acde658a95bb8a038cefdade703da5 | 27 | [3] | None | Zhong, Ji, Xiao, Li, Deng, Chua | DBLP, ArXiv, ACL, DOI, CorpusId |\n",
       "\n",
       "Note:\n",
       "\n",
       "[1] Lewis, S., & Pontus, L. (2021). Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 86‚Äì96.\n",
       "\n",
       "[2] Cambazoglu, B., Sanderson, M., Scholer, J., & Croft, W. (2021). A review of public datasets in question answering research. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 28‚Äì38.\n",
       "\n",
       "[3] Zhong, Y., Ji, W., Xiao, J., Li, Y., Deng, W., & Chua, T. (2022). Video Question Answering: Datasets, Algorithms and Challenges. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 432‚Äì442."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_response(response_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8b11802",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_space = \"effects of caffeine on sleep\" \n",
    "query_string = query_space\n",
    "\n",
    "response_mistral = get_answer_from_model(\"mistral\", query_space, query_string)\n",
    "response_llama = get_answer_from_model(\"llama2:13b\", query_space, query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c85ed149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Caffeine has been shown to have various effects on sleep, including reducing fatigue in sleep-deprived individuals and delaying the onset of sleepiness. In a study by Kim and Wong (2005), they found that caffeine can reduce cognitive dysfunction induced by sleep deprivation. Additionally, Cielinski et al. (2008) found that caffeine can be effective in reducing fatigue in sleep-deprived mice. However, it is important to note that these studies were conducted on animals and further research is needed to determine the effects of caffeine on sleep in humans.\n",
       "Sources:\n",
       "Source 1: Kim, H., & Wong, S. (2005). The Effects of caffeine on cognitive dysfunction induced by sleep deprivation. Journal of Sleep Research, 14(3), 1-8. <https://doi.org/10.1080/1367928050000918>\n",
       "Source 2: Snel, J., Tieges, Z., & Lorist, M. (2004). Effects of Caffeine on Sleep and Wakefulness: An Update. Journal of Clinical Sleep Medicine, 10(6), 593-598. <https://doi.org/10.1201/9780203618851-9>\n",
       "Source 3: Zielinski, M., Carmichael, M., Murphy, E., Youngstedt, S., & Davis, J. (2008). Effects of Caffeine on Exercise Fatigue in Sleep Deprived Mice. <https://doi.org/10.1096/fasebj.22.1_supplement.946.1>\n",
       "Source 4: Daniello, A., Fievisohn, E., & Gregory, T. (2012). Modeling the effects of caffeine on the sleep/ wake cycle. <https://doi.org/10.11259/biomedical.8267301.000000000>\n",
       "Source 5: Effects of Caffeine on Exercise Fatigue in Sleep Deprived Mice. Zielinski, M., Carmichael, M., Murphy, E., Youngstedt, S., & Davis, J. (2008). <https://doi.org/10.1096/fasebj.22.1_supplement.946.1>\n",
       "\n",
       "Source 5: Modeling the effects of caffeine on sleep/wake cycle. Daniello, A., Fievisohn, E., & Gregory, T. (2012). <https://doi.org/10.1089/biomed.2012.643597>\n",
       "\n",
       "According to the available sources, caffeine has effects on sleep and wakefulness. The studies suggest that caffeine can delay the onset of sleepiness and cause sleep disturbances, as it is an adenosine antagonist and inhibits some hormones that promote sleep (Source 2). Caffeine intake late in the evening can also cause the switch from wake to sleep to occur later than if no caffeine was present in the system (Source 5). However, it is important to note that the effects of caffeine on sleep may vary depending on individual factors such as dose, timing, and sensitivity. Additionally, more research is needed to fully understand the mechanisms underlying these effects and to validate the models proposed for incorporating caffeine's impact on sleep (Source 3 and 5)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response_mistral.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79b1e340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Based on the sources you provided, here are the effects of caffeine on sleep:\n",
       "\n",
       "1. Caffeine can delay the onset of sleepiness and cause sleep disturbances (Source 4).\n",
       "2. Sleep deprivation significantly reduces run-time to fatigue, and caffeine attenuates this effect (Source 3).\n",
       "3. Caffeine has a masking effect on the homeostatic drive, promoting wakefulness (Source 4).\n",
       "4. Incorporating the effects of caffeine on the sleep/wake cycle can be modeled using the \"flip-flop\" model (Source 4).\n",
       "5. Caffeine intake late in the evening can cause the switch from wake to sleep to occur later than if no caffeine was present in the system (Source 4).\n",
       "6. The effects of caffeine on sleep following sleep deprivation have not been well-studied, but previous studies suggest that caffeine may reduce sleep quality and duration (Source 5).\n",
       "\n",
       "In summary, caffeine can disrupt sleep patterns and reduce the quality of sleep, especially after sleep deprivation. However, more research is needed to fully understand the effects of caffeine on sleep following sleep deprivation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response_llama.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4f7c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_space = \"effects of drinking too much caffeine\" \n",
    "query_string = query_space\n",
    "\n",
    "response_mistral = get_answer_from_model(\"mistral\", query_space, query_string)\n",
    "response_llama = get_answer_from_model(\"llama2:13b\", query_space, query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "259083e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```scss\n",
       "\"Effects of Drinking Too Much Caffeine\" \"What is the Effects of Drinking Too Much Caffeine\" \"Caffeine Toxicity and Its Effects on the Body\" \"The Dosage of Caffeine That Causes Poisoning\" \"How Long Does it Take for Caffeine to Leave Your System\" \"Does Consuming Large Amounts of Caffeine Help You Lose Weight?\" \"What Are the Side Effects of Drinking Too Much Coffee or Tea?\" \"Can Excessive Caffeine Intake Lead to Sleep Disorders?\" \"How does Drinking Too Much Caffeine Affect Mental Health?\" \"Does Drinking Too Much Caffeine Increase Your Risk of Heart Disease?\" \"What are the Signs and Symptoms of a Caffeine Overdose?\"\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response_mistral.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33e1b081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Based on the provided sources, here are the potential adverse effects of drinking too much caffeine:\n",
       "\n",
       "1. Teratogenic effects: According to Source 3, high single daily doses of caffeine can have teratogenic effects in animals, but not in humans due to the low peak blood levels produced even with high caffeine intakes.\n",
       "2. Maternal toxicity: The same source states that the teratogenic activity is dependent on the production of high peak blood levels, which can cause other signs of severe maternal toxicity.\n",
       "3. Increased homocysteine levels: Source 5 shows that drinking black tea can cause an acute increase in plasma total homocysteine concentrations (tHcy) in human subjects, which may be a risk factor for cardiovascular disease (CVD).\n",
       "4. Sugar intake: Source 1 mentions that consuming high amounts of caffeine may lead to an increased sugar intake, which can have negative health effects when consumed excessively.\n",
       "5. Sleep disturbances: According to Source 2, caffeine consumption can cause sleep disturbances and reduce the total sleep time, especially when consumed in large amounts or close to bedtime.\n",
       "6. Anxiety and jitteriness: The same source states that high caffeine intake can also cause anxiety and jitteriness, which may negatively impact daily activities and relationships.\n",
       "7. Increased heart rate and blood pressure: Source 4 reports that caffeine can increase heart rate and blood pressure, which may be a concern for individuals with pre-existing cardiovascular conditions or hypertension.\n",
       "8. Dehydration: Drinking too much caffeine can lead to dehydration due to the diuretic effects of caffeine, as mentioned in Source 1.\n",
       "9. Interference with medication: According to Source 5, caffeine may interfere with the absorption and efficacy of certain medications, such as antidepressants, and cause unwanted side effects.\n",
       "10. Addiction: Consuming high amounts of caffeine regularly can lead to physical dependence and addiction, making it difficult to stop or reduce consumption, as mentioned in Source 3."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response_llama.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b11902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
