# üìöü§ñ S2QA: Question Answering on research papers from Semantic Scholar

[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://s2questionanswering.streamlit.app)


![Demo](assets/s2qa.gif)

Have you ever wondered what research papers have to say about your burning questions? Look no further than Semantic Scholar Q&A with GPT-4! üôå

This Python script lets you enter a question, and it uses the power of Semantic Scholar and OpenAI to generate an answer based on the content of the top research papers. ü§ñüîç

## üöÄ What's New!

- [Ollama](https://github.com/jmorganca/ollama) support. See [notebook](notebooks/ollama.ipynb)
- S2QA now supports llama-index! Check out the data loader here on [llama hub](https://llamahub.ai/l/semanticscholar)
- Updated UI with chat interface:
  - Keeps the context
  - Shows only used references
  - Index once for each research space; vectors and index are cached
- Full-text support! 


## üíª Demo Link : 

[https://shauryr.github.io/s2qa-demo/](https://shauryr.github.io/s2qa-demo/)

----


## üëè Acknowledgements 
This project is in collaboration with the Semantic Scholar Team. I am thankful for their support and feedback.

----


## üìò Notebooks 
- [ollama.ipynb](notebooks/ollama.ipynb) ‚≠ê Ollama support demo
- [chat_qa.ipynb](notebooks/chat_qa.ipynb) ‚≠ê sends the context to ChatGPT(using revChatGPT) for generating answers. This gives the best answers and is free. Seems less prone to hallucinations than other pipelines in this repo. 
- [utils.py](notebooks/utils.py) - has all the necessary functions for search and GPT-3  prompting
- [s2qa_sources_langchain.ipynb](notebooks/s2qa_sources_langchain.ipynb) - Get better answers with langchain mapreduce but this is very expensive. Prone to hallucinations.
- [s2qa_nb.ipynb](notebooks/s2qa_nb.ipynb) - first notebook. Very basic QA. Prone to hallucinations.

# üö® Caution ‚ö†Ô∏è
Caution should be exercised when using generative AI for scientific text, as the output generated by such models may not be rigorously fact-checked or peer-reviewed, and may therefore contain errors or inaccuracies that could mislead researchers and readers. Be responsible and use this tool with caution.

[![Star History Chart](https://api.star-history.com/svg?repos=shauryr/s2qa&type=Date)](https://star-history.com/#shauryr/s2qa&Date)

----

## ü§ñ Examples

#### ‚ùì Question

#### what is the impact of transformers on language

#### ü§ñ ChatGPT Response:

Transformers have had a significant impact on language in a variety of ways. One of the most prominent areas of impact is in natural language processing (NLP), which has been revolutionized by the development of transformer-based language models. These models have been used for a variety of NLP tasks, including machine translation [[1](https://www.semanticscholar.org/paper/3d67a1fd78c7e74813e898b67fa5965a7d2ce25e)], code summarization [[2](https://www.semanticscholar.org/paper/761472e1c6012e001438d22ef771a0defbac3695)], detecting patronizing and condescending language [[3](https://www.semanticscholar.org/paper/36f7da6d848070f74b48bdf2accace00a66202a3)], and spoken language understanding tasks [[4](https://www.semanticscholar.org/paper/0de905634bd3d42a612ffab7ddd7a814e7e655bb)].

Moreover, transformer-based models have been shown to be effective in domain adaptation for bidirectional transformers [[7](https://www.semanticscholar.org/paper/314107f8ff8bc134703c49c012ea6f6d9eb17840)], multi-modal transformer models have been explored for image captioning [[8](https://www.semanticscholar.org/paper/bec1f88b86bd06e90e2d735be9c54f890ca5c66e)], and pre-training objectives have been used in software engineering to boost performance in code-related tasks [[6](https://www.semanticscholar.org/paper/2e761fe1771d83962eacb7acc473d132bf92b1d3)]. Additionally, transformers have also been used to incorporate document timestamps into language models, which has shown to have a positive impact on downstream classification tasks and masked language modeling [[5](https://www.semanticscholar.org/paper/a702e0a38086c54c65ef7f2290eea726e13e0ca7)].

Overall, transformers have had a significant impact on language, particularly in the field of natural language processing, where transformer-based models have been used for a variety of tasks with great success. They have also been explored in other areas such as software engineering and image captioning, showing the versatility of the technology.

#### [1]  [A Study on the journey of Natural Language Processing models: from Symbolic Natural Language Processing to Bidirectional Encoder Representations from Transformers](https://www.semanticscholar.org/paper/3d67a1fd78c7e74813e898b67fa5965a7d2ce25e)  - International Journal of Scientific Research in Computer Science Engineering and Information Technology, 2021

#### [2]  [Code Summarization: Do Transformers Really Understand Code?](https://www.semanticscholar.org/paper/761472e1c6012e001438d22ef771a0defbac3695)  - , 2022

#### [3]  [PALI-NLP at SemEval-2022 Task 4: Discriminative Fine-tuning of Transformers for Patronizing and Condescending Language Detection](https://www.semanticscholar.org/paper/36f7da6d848070f74b48bdf2accace00a66202a3)  - International Workshop on Semantic Evaluation, 2022

#### [4]  [Benchmarking Transformers-based models on French Spoken Language Understanding tasks](https://www.semanticscholar.org/paper/0de905634bd3d42a612ffab7ddd7a814e7e655bb)  - Interspeech, 2022

#### [5]  [Temporal Language Modeling for Short Text Document Classification with Transformers](https://www.semanticscholar.org/paper/a702e0a38086c54c65ef7f2290eea726e13e0ca7)  - Conference on Computer Science and Information Systems, 2022

#### [6]  [Automating Code-Related Tasks Through Transformers: The Impact of Pre-training](https://www.semanticscholar.org/paper/2e761fe1771d83962eacb7acc473d132bf92b1d3)  - ArXiv, 2023

#### [7]  [Exploiting Auxiliary Data for Offensive Language Detection with Bidirectional Transformers](https://www.semanticscholar.org/paper/314107f8ff8bc134703c49c012ea6f6d9eb17840)  - WOAH, 2021

#### [8]  [What Does a Language-And-Vision Transformer See: The Impact of Semantic Information on Visual Representations](https://www.semanticscholar.org/paper/bec1f88b86bd06e90e2d735be9c54f890ca5c66e)  - Frontiers in Artificial Intelligence, 2021

---

## ü§ñ Answers with sources and langchain mapreduce

<!-- <img src="https://github.com/shauryr/S2QA/blob/master/demo.jpg" alt="s2 with langchain and sources" width="500"> -->

![Demo](assets/demo.jpg)

## üß∞ Requirements 

- `OpenAI API key` (if you are using langchain)
- `Semantic Scholar Academic Graph API key` - https://www.semanticscholar.org/product/api

These can be added in the [constants.py](notebooks/constants.py)

The main third-party package requirements are `tiktoken`, `openai`, `transformers` and `langchain`.

To install all the required packages
```
pip install -r requirements.txt
```

## üìç Pipeline 

1Ô∏è‚É£ `Searching` : We begin by searching the vast and ever-growing database of Semantic Scholar to find the most up-to-date and relevant papers and articles related to your question.

2Ô∏è‚É£ `Re-Ranking` : We then use [SPECTER](https://github.com/allenai/specter) to embed these papers and re-rank the search results, ensuring that the most informative and relevant articles appear at the top of your search results.

3Ô∏è‚É£ `Answering` : Finally, we use the powerful natural language processing capabilities of GPT-3 to generate informative and accurate answers to your question, using custom prompts to ensure the best results.

## üñäÔ∏è Customizable 

- Try other open embedding methods on huggingface to see better re-ranking results. 

- Try other prompts or refine the current prompt to get even better answers.

