{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f823fa1",
   "metadata": {},
   "source": [
    "### SAP Machine Learning Embedding in OpenAI\n",
    "##### Author: Sergiu Iatco. July, 2023\n",
    "https://people.sap.com/iatco.sergiu <br>\n",
    "https://www.linkedin.com/in/sergiuiatco/ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a62a4",
   "metadata": {},
   "source": [
    "#### Resources:\n",
    "https://pypi.org/project/gpt-index/ <br>\n",
    "https://github.com/jerryjliu/llama_index/blob/main/examples/langchain_demo/LangchainDemo.ipynb <br>\n",
    "https://github.com/jerryjliu/llama_index/tree/main/examples <br>\n",
    "https://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/SimpleIndexDemo-ChatGPT.ipynb <br>\n",
    "https://gpt-index.readthedocs.io/en/stable/reference/service_context.html <br>\n",
    "https://gpt-index.readthedocs.io/en/stable/reference/service_context/embeddings.html <br>\n",
    "https://gpt-index.readthedocs.io/en/stable/getting_started/starter_example.html store and load <br>\n",
    "https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html <br>\n",
    "\n",
    "https://blog.streamlit.io/how-to-build-an-llm-powered-chatbot-with-streamlit/ <br>\n",
    "https://github.com/dataprofessor/langchain-text-summarization <br>\n",
    "https://github.com/dataprofessor <br>\n",
    "\n",
    "Blogs: <br>\n",
    "https://blogs.sap.com/2022/11/07/sap-community-call-sap-hana-cloud-machine-learning-challenge-i-quit-how-to-prevent-employee-churn/ <br>\n",
    "https://blogs.sap.com/2022/11/28/i-quit-how-to-predict-employee-churn-sap-hana-cloud-machine-learning-challenge/ <br>\n",
    "https://blogs.sap.com/2022/12/22/sap-hana-cloud-machine-learning-challenge-2022-the-winners-are/ <br>\n",
    "\"I quit!\" - How to prevent employee churn | SAP Community Call | Kick-off <br>\n",
    "https://www.youtube.com/watch?v=pgV_NFdokZ4 <br>\n",
    "\"How to prevent Employee Churn using SAP HANA Cloud | SAP Community Call | Solutions\" <br>\n",
    "https://www.youtube.com/watch?v=ul5ZqnB3qVw <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc4bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac381f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.core.debugger import set_trace\n",
    "# os.environ[\"OPENAI_API_KEY\"] = '<OPENAI_API_KEY>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd5a045",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.CRITICAL)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# There are five standard levels for logging in Python, listed here in increasing order of severity:\n",
    "# DEBUG: Detailed information, typically of interest only when diagnosing problems.\n",
    "# INFO: Confirmation that things are working as expected.\n",
    "# WARNING: An indication that something unexpected happened or indicative of some problem in the near future (e.g., ‘disk space low’). The software is still working as expected.\n",
    "# ERROR: Due to a more serious problem, the software has not been able to perform some function.\n",
    "# CRITICAL: A very serious error, indicating that the program itself may be unable to continue running.\n",
    "\n",
    "class llama_context():\n",
    "    def __init__(self, path=None):\n",
    "        \n",
    "        if path!=None:\n",
    "            self.path = path\n",
    "        else:\n",
    "            self.path = ''\n",
    "        \n",
    "        perisit_sub_dir = \"storage\"\n",
    "        self.perisit_dir = os.path.join(self.path, perisit_sub_dir)\n",
    "        if not os.path.exists(self.perisit_dir):\n",
    "            os.makedirs(self.perisit_dir)\n",
    "        data_sub_dir = \"data\"\n",
    "        self.data_dir = os.path.join(self.path, data_sub_dir)\n",
    "        self.data_dir_counter = 0\n",
    "        \n",
    "        self.cost_model_ada = \"ada\" # https://openai.com/pricing\n",
    "        self.cost_model_davinci = \"davinci\" # https://openai.com/pricing\n",
    "        self.price_ada_1k_tokens = 0.0004\n",
    "        self.price_davinci_1k_tokens = 0.03 \n",
    "\n",
    "        \n",
    "    def load_data(self):\n",
    "        self.documents = SimpleDirectoryReader(self.data_dir).load_data()\n",
    "        print(f\"Documents loaded: {len(self.documents)}.\")\n",
    "    def create_vector_store(self):\n",
    "        self.index = GPTVectorStoreIndex.from_documents(self.documents)\n",
    "        print(\"GPTVectorStoreIndex complete.\")\n",
    "    def save_index(self):\n",
    "        self.index.storage_context.persist(persist_dir=self.perisit_dir)\n",
    "        print(f\"Index saved in path {self.perisit_dir}.\")\n",
    "    def load_index(self):\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=self.perisit_dir)\n",
    "        self.index = load_index_from_storage(storage_context)\n",
    "    def start_query_engine(self):\n",
    "        self.query_engine = self.index.as_query_engine()\n",
    "        print(\"Query_engine started.\")\n",
    "    def post_question(self, question, sleep = None):\n",
    "        if sleep == None:\n",
    "            self.sleep = 0 # trial 20s\n",
    "        self.response_cls = self.query_engine.query(question)\n",
    "        self.response = self.response_cls.response\n",
    "\n",
    "    def del_data_dir(self):\n",
    "        path = self.data_dir\n",
    "        try:\n",
    "            shutil.rmtree(path)\n",
    "            print(f\"{path} deleted successfully!\")\n",
    "        except OSError as error:\n",
    "            print(f\"Error deleting {path}: {error}\")\n",
    "\n",
    "    def copy_file_to_data_dir(self, file_extension ='.txt', verbose = 0):\n",
    "\n",
    "        path_from = self.path\n",
    "        path_to = self.data_dir\n",
    "\n",
    "        if not os.path.exists(path_to):\n",
    "            os.makedirs(path_to)\n",
    "\n",
    "        for filename in os.listdir(path_from):\n",
    "            if filename.endswith(file_extension):\n",
    "                source_path = os.path.join(path_from, filename)\n",
    "                dest_path = os.path.join(path_to, filename)\n",
    "                shutil.copy(source_path, dest_path)\n",
    "                if verbose == 1:\n",
    "                    print(f\"File {filename} copied successfully!\")\n",
    "    \n",
    "        path_to_lib = pathlib.Path(path_to)\n",
    "        path_to_lib_files = path_to_lib.glob(f\"*{file_extension}\")\n",
    "        print(f\"Files {len(list(path_to_lib_files))} copied in {path_to}.\")\n",
    " \n",
    "    def copy_path_from_to_data_dir(self, path_from, file_extension ='.txt', verbose = 0):\n",
    "\n",
    "        path_to = self.data_dir # default data folder for llama\n",
    "        start_counter = self.data_dir_counter\n",
    "        \n",
    "        if not os.path.exists(path_to):\n",
    "            os.makedirs(path_to)\n",
    "\n",
    "        padding_n = 5\n",
    "        path_from_lib = pathlib.Path(path_from)\n",
    "        path_from_lib_files = path_from_lib.glob(f\"**/*{file_extension}\")\n",
    "\n",
    "        files_copied_n = 0\n",
    "        counter = None\n",
    "        for counter, file in enumerate(path_from_lib_files, start_counter):\n",
    "            filename_path = os.path.split(file)[0] # path only\n",
    "            filename = os.path.split(file)[1] # filename only\n",
    "            filename_with_index = f'{str(counter).zfill(padding_n)}_{filename}'\n",
    "            file_to_data_dir = os.path.join(path_to, filename_with_index)\n",
    "            shutil.copy(file, file_to_data_dir)\n",
    "            \n",
    "            if os.path.exists(file_to_data_dir):\n",
    "                files_copied_n += 1\n",
    "                if verbose == 1:\n",
    "                    print(f\"File {filename} -> copied successfully!\")\n",
    "            else:\n",
    "                if verbose == 1:\n",
    "                    print(f\"File {filename} was not copied!\")\n",
    "        \n",
    "#         if 'counter' in locals(): \n",
    "        if counter != None: \n",
    "            self.data_dir_counter = counter + 1 # start from last\n",
    "        \n",
    "        print(f\"Files: {files_copied_n} copied to folder: {path_to}!\")\n",
    "\n",
    "    def estimate_tokens(self, text):\n",
    "        words = text.split()\n",
    "\n",
    "        num_words = int(len(words))\n",
    "        tokens = int(( num_words / 0.75 ))\n",
    "        tokens_1k = tokens / 1000\n",
    "        cost_ada = tokens_1k * self.price_ada_1k_tokens\n",
    "        cost_davinci = tokens_1k * self.price_davinci_1k_tokens\n",
    "        return tokens, cost_ada, cost_davinci\n",
    "    \n",
    "    def estimate_cost(self):\n",
    "        total_tokens = 0\n",
    "        total_cost_ada = 0\n",
    "        total_cost_davinci = 0\n",
    "        costs_rounding = 8\n",
    "        \n",
    "        for doc in self.documents:\n",
    "            text = doc.get_text()\n",
    "            tokens, cost_ada, cost_davinci = self.estimate_tokens(text)\n",
    "            total_tokens += tokens\n",
    "            \n",
    "            total_cost_ada += cost_ada\n",
    "            total_cost_ada = round(total_cost_ada, costs_rounding)\n",
    "            \n",
    "            total_cost_davinci += cost_davinci\n",
    "            total_cost_davinci = round(total_cost_davinci, costs_rounding)\n",
    "            \n",
    "        self.total_tokens = total_tokens\n",
    "        self.total_cost_ada = total_cost_ada\n",
    "        self.total_cost_davinci = total_cost_davinci\n",
    "        print(f\"Total tokens: {self.total_tokens}\")\n",
    "        print(f\"Total estimated costs with model {self.cost_model_ada }: ${self.total_cost_ada}\")\n",
    "        print(f\"Total estimated costs with model {self.cost_model_davinci }: ${self.total_cost_davinci}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78c84ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "YoutubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\")\n",
    "loader = YoutubeTranscriptReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f43797eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytb_name = 'ytb_hana_ml_call_20221128'\n",
    "ytb_link = 'https://www.youtube.com/watch?v=pgV_NFdokZ4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ea276e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and we're live\n",
      "perfect so please take it away hi\n",
      "everyone and a warm welcome to today's\n",
      "sap Community call my name is Susan and\n",
      "I am proud of the sap Hana product\n",
      "management team and I will be your host\n",
      "today and I'm really really excited\n",
      "about the topic of today's call so we\n",
      "will first get an introduction to\n",
      "machine learning in sap Hana cloud and\n",
      "afterwards we'll finally kick off our\n",
      "sap Hana 12 machine learning challenge\n",
      "so thanks already for joining here and\n",
      "your big interest in the challenge so\n",
      "let me introduce the most important\n",
      "people today we're having in the back\n",
      "end with me Savage whose competency for\n",
      "data science and machine learning at sap\n",
      "Andreas Foster our machine learning\n",
      "expert in Sap's Global Center of\n",
      "Excellence Yannick sharp our customer\n",
      "advisor for machine learning and my\n",
      "fellow product manager Christoph Morgan\n",
      "senior director of product management\n",
      "sap Hana predictive and machine learning\n",
      "so our call will be 60 Minutes long and\n",
      "the recording will be available under\n",
      "the exact link you're joining us today\n",
      "so one last thing before I hand it over\n",
      "to our speakers make sure to use our q a\n",
      "tools so I'm right next to the video in\n",
      "YouTube you see the chat window just\n",
      "post all your questions in there and\n",
      "we're hopefully having enough time at\n",
      "the end of our call to answer all your\n",
      "questions either in a written format in\n",
      "the chat or live in a spoken format so\n",
      "with this Christoph I don't want to\n",
      "steal any more time from you I'm handing\n",
      "it over\n",
      "thank you very much Susan\n",
      "for the nice introduction and also warm\n",
      "welcome from my side to our Hana machine\n",
      "Learning Community chilling uh challenge\n",
      "I'm really excited we have been able to\n",
      "put this together with our machine\n",
      "learning experts from the field\n",
      "um we'll also have uh put to put\n",
      "together the the challenge itself\n",
      "and yeah basically as you already kind\n",
      "of outlined what we want to do in the\n",
      "next couple of minutes quickly I will\n",
      "introduce the the Hana ml capabilities\n",
      "and overview the topic very briefly\n",
      "then Yannick will give you a demo\n",
      "overview on how to make use of the Hana\n",
      "machine learning capabilities\n",
      "and\n",
      "um yeah and again Sarah will then\n",
      "describe the challenge itself the the\n",
      "topic which is the key use case for for\n",
      "our Challenge and also Andreas will then\n",
      "kind of outline how we organize the\n",
      "overall uh Challenge from our process\n",
      "perspective when we meet how we meet uh\n",
      "how we can interact and yeah how you can\n",
      "get access to the the Hana Cloud\n",
      "challenge\n",
      "also here's uh uh some some pictures of\n",
      "all of us across the regional experts\n",
      "which you which will be joining our open\n",
      "office called uh regularly uh in the\n",
      "different regions will also come to them\n",
      "and introduce them as well so\n",
      "Hana machine learning is one of the key\n",
      "capabilities of the Hana cloud or the\n",
      "Hana database in in general along with\n",
      "other Advanced processing capabilities\n",
      "like spatial graph or storing data in\n",
      "the Json documents or some of the\n",
      "extended key capabilities which make\n",
      "Hana let's say a unique and Rich\n",
      "multi-model database and you can build\n",
      "your applications on btp on sap Hana\n",
      "Cloud for example and directly Infuse\n",
      "them with those those Advanced\n",
      "capabilities without the needs need to\n",
      "add another spatial application server\n",
      "or machine learning\n",
      "let's say infrastructure it's all part\n",
      "of Hana cloud in the first place and\n",
      "that's why it makes this so attractive\n",
      "to yeah apply machine learning scenarios\n",
      "directly with sap Hana\n",
      "uh we will also showcase our native\n",
      "client interfaces for data scientists in\n",
      "Iron python so in case you don't know\n",
      "them uh yet we will showcase them to you\n",
      "we have two key AI function libraries\n",
      "embedded in Hana Cloud the first one is\n",
      "the automated predictive Library which\n",
      "targets yeah the classic machine\n",
      "learning scenarios like classification\n",
      "regression and Times News forecasting\n",
      "some others and the key value of the\n",
      "automated predictive library is that\n",
      "there's a simple set of functions which\n",
      "everybody can use non-data scientists\n",
      "developers\n",
      "administrators of a Hana system can\n",
      "really use because the engine automates\n",
      "the complete step from accessing my data\n",
      "selecting the best variables data\n",
      "preparation maybe encoding even of uh\n",
      "distinct columns missing value handling\n",
      "all that and comes up with the best\n",
      "possible model that's why it's very very\n",
      "popular uh yeah AI function library\n",
      "because it provides you with a high\n",
      "productivity and fast time to Value to\n",
      "kind of come up with a really really\n",
      "good machine learning model in the first\n",
      "place\n",
      "our second Library more for the expert\n",
      "is the predictive analysis Library it\n",
      "provides education for the same or\n",
      "similar scenarios like the automated\n",
      "predictive library but those are kind of\n",
      "distinct algorithms the experts can\n",
      "choose from like for example gradient\n",
      "boosting classification or profit time\n",
      "series forecasting so it provides\n",
      "trending algorithm along with standard\n",
      "and classic algorithm in those\n",
      "distinctive domains\n",
      "plus adding uh your expert support\n",
      "capabilities like uh segmented\n",
      "forecasting invocation explainability\n",
      "for model interpretability\n",
      "or uh yeah hyperscaler not hyperscaler\n",
      "hyper parameter model selection and\n",
      "capabilities like that it's also driven\n",
      "by the SQL interface as well as the r\n",
      "and python machine learning clients and\n",
      "here\n",
      "you see that there's a rich set of\n",
      "functions and algorithms available with\n",
      "the predictive analysis Library it uh it\n",
      "is distinguished a little bit between\n",
      "the on-premise sap Hana and the sap Hana\n",
      "Cloud here in our challenge we are\n",
      "sharing a Hana Cloud instance which has\n",
      "all the algorithms you see on the slide\n",
      "available\n",
      "which you can choose and make use of as\n",
      "part of the the challenge everything\n",
      "bluish and black is also available uh\n",
      "within johana on premise installation\n",
      "um so a rich set and maybe Swiss army\n",
      "knife to with all kinds of algorithms\n",
      "and the best is as a data scientist and\n",
      "as you may already know those algorithms\n",
      "you can leverage them via the pisman are\n",
      "machine learning clients where you\n",
      "basically script in those languages\n",
      "but you run in Hana as the interface\n",
      "generates secret on the Fly and execute\n",
      "with sabiana data is exposed by a data\n",
      "frame concept similar to what you may\n",
      "already know in Python R and then the\n",
      "methods are wrapped in python or R and\n",
      "you can then script\n",
      "um with your python Jupiter notebook\n",
      "like you would be scripting with\n",
      "scikit-learn for example everything is\n",
      "really similar and close so the\n",
      "experience and productivity for you or a\n",
      "data scientist is really good\n",
      "so these are our machine learning\n",
      "clients and I guess this was the the\n",
      "brief info from my side and I think\n",
      "what's most interesting how uh\n",
      "does it look like live and I think\n",
      "Yannick is now giving you the demo\n",
      "experience\n",
      "perfect thank you Chris\n",
      "so let me share my screen and\n",
      "before we jump into the tool and my\n",
      "python environment I want to first show\n",
      "you a small architecture of the demo and\n",
      "what we will see in the next couple of\n",
      "minutes so I I will be working from my\n",
      "local Jupiter lab environment but you\n",
      "can of course also use other pricing\n",
      "environments to leverage our Hana\n",
      "machine learning capabilities\n",
      "and what I will do first is install the\n",
      "Hana ml package which will contain the\n",
      "hair functionalities Crystal introduced\n",
      "and I will then be able to use them\n",
      "from my local python environment without\n",
      "moving the data into my my local\n",
      "environment so I can\n",
      "stay here in my python environment but\n",
      "still use the functionalities in Hana\n",
      "and the data can stay where it is so\n",
      "really we brought the algorithms to the\n",
      "data\n",
      "and with these functionalities I will\n",
      "then show you how I can predict a\n",
      "quality of a production\n",
      "and in my demo and yeah I cannot see you\n",
      "screaming Susan also cannot gets new\n",
      "screen mature that's a general issue\n",
      "maybe you can re-share again oh sorry\n",
      "about that no problem\n",
      "yeah it's\n",
      "ski\n",
      "I mean this is life it says maybe\n",
      "somebody can take\n",
      "can take over the sharing and then I'll\n",
      "restart again because it's a whole Zoom\n",
      "is kept Frozen sorry about that\n",
      "no no problem maybe I can find Andreas\n",
      "do you wanna\n",
      "go ahead with kind of your part\n",
      "are you ready then we come back to the\n",
      "demo once uh the unexpect\n",
      "I was hoping to talk over this night\n",
      "that yeah so\n",
      "yeah but now we can do it it's working\n",
      "yeah can you see my screen now yes we\n",
      "can okay okay sorry about that some some\n",
      "technical problems\n",
      "um yeah so I hope you can see now my my\n",
      "architecture so to maybe summarize that\n",
      "I'll stay in my local Jupiter\n",
      "environment my low comparison\n",
      "environment you can of course use also\n",
      "use other python environment\n",
      "um to install the Hana ml package from\n",
      "there you can leverage them these\n",
      "functionalities in Hana cloud\n",
      "and I would say let's let's take a look\n",
      "at the concrete example and we will of\n",
      "course also find the code I'm about to\n",
      "share in a blog post we published in in\n",
      "a GitHub repository so if you want to\n",
      "follow\n",
      "um demo later in through the blog post\n",
      "that is of course also possible\n",
      "okay\n",
      "so I hope you can see now my local\n",
      "Jupiter environment\n",
      "and\n",
      "I said the first thing you have to\n",
      "install is the Hana package which\n",
      "leverages the functionality is Crystal\n",
      "introduced\n",
      "through so through this package I am now\n",
      "able to use the predictive analyzers\n",
      "library and auto made a predictive\n",
      "library and connect to my Hana cloud\n",
      "system\n",
      "so I installed it and uh then imported\n",
      "and now of course the next step is to\n",
      "actually connect to my\n",
      "um Hana system and you have different\n",
      "options to do this so\n",
      "for the challenge\n",
      "um you might have already tested the\n",
      "connection from your python environment\n",
      "but one of the scripts in the git\n",
      "Repository\n",
      "you're able to copy the credentials into\n",
      "the fields and then of course yeah test\n",
      "the connection\n",
      "but of course you can also hide your\n",
      "credentials and don't have them visible\n",
      "in in your cell as script and I've for\n",
      "this I've added a security key which is\n",
      "also described in the blog post which\n",
      "could contains my credentials and now\n",
      "I'm able to connect to my Hana cloud\n",
      "system through this security key\n",
      "and this connection variable here will\n",
      "then be very important for me to yeah\n",
      "create the Hana data frames and to of\n",
      "course then save my results in my Hana\n",
      "Cloud later on\n",
      "and\n",
      "now there are two options so the first\n",
      "scenario could be that the data is\n",
      "currently locally here on my laptop and\n",
      "I want to upload it into my Hana cloud\n",
      "system\n",
      "so for this demo I've prepared some some\n",
      "data which is of course not the\n",
      "challenge we didn't want to make it that\n",
      "that simple\n",
      "and I've yeah then trans transformed\n",
      "this later this data a little bit added\n",
      "in ide and then I will pushing it now\n",
      "directly into my Hana cloud system into\n",
      "my schema\n",
      "and you would be able to do this as well\n",
      "you can take the data from The Challenge\n",
      "load it locally and then push it into\n",
      "the Hana cloud system\n",
      "for example is providing to you\n",
      "then later on the data is of course\n",
      "already yeah lying in your Hana cloud\n",
      "system and you don't always want to push\n",
      "it into Hana every single time when you\n",
      "create the Hana data frame and of course\n",
      "in reality\n",
      "often the data is already in your\n",
      "backend system in this hopefully not\n",
      "um only locally on your laptop\n",
      "and I can create this Hana data frame\n",
      "also in a couple of different ways\n",
      "without moving the data into my local\n",
      "environment and one option you see here\n",
      "in the script\n",
      "so remember I created this connection\n",
      "variable\n",
      "and I now have the option to execute an\n",
      "SQL statement very simple here which\n",
      "just points to the data in my schema in\n",
      "my Hana cloud\n",
      "and with with this I've now created a\n",
      "Hana data frame\n",
      "with which I can now work I can do data\n",
      "understanding I can do data preparation\n",
      "and I can also Leverage The Machine\n",
      "learning capabilities without moving\n",
      "this data into my local environment so\n",
      "it it's kind of like an arrow\n",
      "pointing to the data in My Hangout file\n",
      "system\n",
      "okay so this is prepared so let's take a\n",
      "look at some data preparation or data\n",
      "understanding steps so for example if\n",
      "you first have to clean your data before\n",
      "you actually can do machine learning and\n",
      "for this you have of course a couple of\n",
      "functions already available here to you\n",
      "through the Hana ml package for example\n",
      "the variable\n",
      "quality which I actually want to predict\n",
      "is in the wrong format and I can get\n",
      "quickly change it into a character\n",
      "format so that the algorithms automatic\n",
      "automatically detect that it's probably\n",
      "a classification what it's supposed to\n",
      "do\n",
      "and of course I can do complex for data\n",
      "preparation steps here as well I could\n",
      "execute of course also SQL scripts to to\n",
      "clean the data\n",
      "um I can also visualize the data so here\n",
      "I'm doing a small description of the\n",
      "data and then I'm only collecting the\n",
      "actual report\n",
      "of this describe method so the heavy\n",
      "lifting stays in Hana and I'm only\n",
      "collecting small samples of the data\n",
      "like this described report into my local\n",
      "environment to visualize it\n",
      "okay now of course in reality data\n",
      "understanding and data preparation takes\n",
      "quite a lot of time\n",
      "but I would like to move on now to\n",
      "um yeah the machine learning part\n",
      "because I think it's very powerful and I\n",
      "can yeah work just like I'm used to with\n",
      "also open source package like cycle\n",
      "learn or other packages to now split the\n",
      "data into a training and the testing set\n",
      "before I train my machine learning\n",
      "algorithm and this is done through\n",
      "this cell so through this command I'm\n",
      "now I'm splitting the data into 80\n",
      "training set and 20 of the data I will\n",
      "preserve for yeah testing my train\n",
      "machine learning modeling\n",
      "I can then control the size of my\n",
      "training and testing set\n",
      "and then I can proceed to\n",
      "um do machine learning and as Christoph\n",
      "mentioned there are a lot of options\n",
      "available now for you to solve the\n",
      "challenge and I will choose one\n",
      "possibility which is an algorithm in the\n",
      "predictive analysis Library\n",
      "and I'm a big fan of the random Forest\n",
      "um which is yeah a great method to you\n",
      "know predict if a transaction if a\n",
      "creative product will be of good quality\n",
      "or bad quality and\n",
      "for this I'm now using the random Forest\n",
      "classifier here and the predictive\n",
      "analyzers library and I first want to\n",
      "train this\n",
      "um model with a lot of threes so\n",
      "um to see kind of how does the error in\n",
      "my algorithm\n",
      "um convert converge and I can optimize\n",
      "this algorithm here just like I'm used\n",
      "to in the open source world so I can now\n",
      "go into more\n",
      "um hyper parameter tuning and it's it's\n",
      "really just like if I would be um\n",
      "optimizing an algorithm from Cycles\n",
      "learn\n",
      "when I've trained the the model I can\n",
      "then create prediction with it\n",
      "um predict\n",
      "um if the product will be of good or bad\n",
      "quality I can look at the score and the\n",
      "confidence how how sure is the algorithm\n",
      "if a product will be of good or bad\n",
      "quality\n",
      "I can further evaluate it so I can for\n",
      "example of course look at the confusion\n",
      "Matrix how good is the algorithm in its\n",
      "decision making this one here is in\n",
      "Sample but I can of course also apply it\n",
      "to the testing data which I've kind of\n",
      "preserved on the side and look at the\n",
      "confusion Matrix out of sample\n",
      "to to control if my machine learning\n",
      "model is actually of good quality\n",
      "and yeah of course then many many more\n",
      "capabilities to actually evaluate\n",
      "um the model of course\n",
      "um random Forest is also a really\n",
      "amazing algorithm because I don't only\n",
      "have the prediction but I also have\n",
      "um kind of more insights\n",
      "um\n",
      "what which variables were of importance\n",
      "which\n",
      "um variables were not as important to\n",
      "predict if a product will be of good or\n",
      "bad quality\n",
      "so I also generate not only the\n",
      "predictions but really further insights\n",
      "for the business\n",
      "and yeah I kind of want to scroll\n",
      "through here so I'm now doing a lot of\n",
      "hyper parameter tuning here and\n",
      "in the end I of course have one final\n",
      "model which I'm yeah which I'm happy\n",
      "with\n",
      "um which I then want to of course also\n",
      "save in my Hana cloud and maybe use it\n",
      "at a later time again and for this I'm\n",
      "now also able to create a model storage\n",
      "directly in in my Hana\n",
      "and in there I can then save my final\n",
      "machine learning model which I've\n",
      "optimized\n",
      "and then later on of course always load\n",
      "it and use it for for predictions and\n",
      "for example in here you see I've already\n",
      "created\n",
      "um two random Forest models which I can\n",
      "then further use\n",
      "now this was just one example how to\n",
      "Leverage The Machine learning\n",
      "capabilities in our cloud and there are\n",
      "of course many more and for you of\n",
      "course it is important then when you\n",
      "start with it how do you find the\n",
      "documentation how to actually\n",
      "um yeah tune such an algorithm\n",
      "and for this I want to quickly show you\n",
      "the\n",
      "[Music]\n",
      "link to the\n",
      "documentation there it is\n",
      "so under the link you will then find of\n",
      "course a lot of more examples and you\n",
      "can you can try and in here you will\n",
      "also have the documentation how you\n",
      "actually yeah tune these algorithms and\n",
      "um you find a lot more possibilities\n",
      "um there as well\n",
      "and with that I would say we are now\n",
      "ready to look at the actual challenge\n",
      "which you will have to solve in this ml\n",
      "Community challenge\n",
      "so Sarah\n",
      "can you tell us a little bit more about\n",
      "the actual challenge\n",
      "sure we prepared a lot of fun things for\n",
      "you\n",
      "and maybe let's start with the concept\n",
      "that we prepared for you for our\n",
      "community challenge so the idea is that\n",
      "we have some hard experimental learning\n",
      "especially for you\n",
      "so Yannick when we move to the next\n",
      "slide\n",
      "yeah seems to have problems again\n",
      "no worries\n",
      "just give it a few seconds\n",
      "so maybe you can take over the share the\n",
      "screen sharing\n",
      "thank you\n",
      "there we go yes we prepared a Hana cloud\n",
      "system for you so you should all have\n",
      "access to that\n",
      "um if you don't it's also described in\n",
      "the blog post in the kickoff blog post\n",
      "how you can get access to it\n",
      "then we have a nice challenge for you I\n",
      "will give you a few more insights about\n",
      "that one later on and as Yannick already\n",
      "mentioned we have some expert content\n",
      "yeah you have a lot of GitHub links you\n",
      "also have some Hands-On tutorials that\n",
      "might help you doing the challenge and\n",
      "in case you get stuck we are of course\n",
      "always there for you so we have some\n",
      "open Office hours twice a week so this\n",
      "is again also a zoom call which is not\n",
      "recorded not live on YouTube yeah so you\n",
      "can be so you can join and ask us all\n",
      "the questions\n",
      "yeah that you want and that might help\n",
      "you solve the challenge in case um you\n",
      "don't want to wait for our open Office\n",
      "hours or something comes up and during\n",
      "these days you can also ask the\n",
      "questions at any time at the SFP\n",
      "Community there you can also see the\n",
      "kickoff blog post and then you can just\n",
      "write your questions as a comment to\n",
      "this blog post now we will be monitoring\n",
      "that and then we'll also try to help you\n",
      "with um\n",
      "this in the sap community\n",
      "yeah the idea is that we all work and\n",
      "learn together yeah so feel free to work\n",
      "in teams if you want if you want to\n",
      "participate alone that's also fine but\n",
      "also we want you to share your\n",
      "experience in our final call where you\n",
      "also can present your results yeah that\n",
      "you shared it also with all the other\n",
      "participants\n",
      "yes so the idea is that everyone can\n",
      "really improve their skills we all can\n",
      "learn something during this Challenge\n",
      "and we can learn as much as we can also\n",
      "from each other\n",
      "I already mentioned we also have a final\n",
      "call where you can all present your\n",
      "results you can you don't have to but we\n",
      "would highly appreciate it if you prefer\n",
      "not to present it during a call you can\n",
      "also write a blog post about it and post\n",
      "it on the sap Community challenge so\n",
      "it's up to you what you want and then we\n",
      "have our experts who then will decide\n",
      "who wins the hackathon now we also have\n",
      "some nice prizes for you\n",
      "so I would say give it your best shot\n",
      "yeah do your very best during your our\n",
      "Challenge and then try to win the\n",
      "hackathon\n",
      "yeah let's maybe talk a little bit more\n",
      "about a challenge\n",
      "and here's our evaluation criteria\n",
      "like I said you have a challenge so you\n",
      "have one use case that you should solve\n",
      "with the sap Hana machine learning\n",
      "Asiana presented to you it's up to you\n",
      "if you want to use the automated\n",
      "predictive library or the predictive\n",
      "analysis library but you should\n",
      "definitely use the things which are\n",
      "available on the Hana cloud\n",
      "and we are also looking for an appealing\n",
      "presentation so give it a clear message\n",
      "and also think about for whom are you\n",
      "building this yes so you should also\n",
      "think about the business context maybe\n",
      "you have a Persona in mind for whom\n",
      "you're building this and then\n",
      "give it a clear message make it really\n",
      "nice presentation and we are not just\n",
      "looking for the highest security\n",
      "position or recall of course this is\n",
      "also important but we are also looking\n",
      "for the best and catchy story yeah so\n",
      "just make it a nice and fun presentation\n",
      "and I think you're good to go\n",
      "okay then on the next slide we have all\n",
      "the details about our challenge\n",
      "yeah here we go so it's all about\n",
      "employee churn now Susan wrote this\n",
      "quite provocatively with I quit yeah we\n",
      "thought you know during the times where\n",
      "everyone is looking for the best talents\n",
      "this is really important you the use\n",
      "case to really yeah\n",
      "retain yeah your most skilled people\n",
      "make him happy so that they stay with\n",
      "you as long as they can\n",
      "so this is then what it's all about yeah\n",
      "you should predict which employees will\n",
      "be leaving the company within the next\n",
      "12 months yeah and\n",
      "yeah I think this is a highly\n",
      "interesting case because when I talk to\n",
      "managers sometimes um they think when\n",
      "someone has a new haircut then that\n",
      "might be the highest indication but we\n",
      "think the data can do better yeah so\n",
      "that's why we prepared a lot of yeah\n",
      "different\n",
      "um\n",
      "yeah data are the different factors for\n",
      "you influencing factors so you can\n",
      "choose the one which really have an\n",
      "influence yeah I hope you remind that\n",
      "having a new haircut or not is not\n",
      "included in the data set because we felt\n",
      "that is not so relevant yeah\n",
      "um so let's have a bit more of a look\n",
      "on the data Kristoff\n",
      "and maybe you can press on the play\n",
      "button so we can see that so this is um\n",
      "actually the data that we prepared for\n",
      "you yeah you see you have a lot of\n",
      "different influencing factors that you\n",
      "can see here it's basically everything\n",
      "so it's the age it's a gender so the\n",
      "classical Master data that we know about\n",
      "our employees but also the things here\n",
      "is it a full-time employee or not\n",
      "did they change already yeah within the\n",
      "companies on which functional area did\n",
      "they work before in which country did\n",
      "they work before how long was have they\n",
      "been with the formal\n",
      "um yeah with the formal position was it\n",
      "an external hire or something and also\n",
      "when they changed the position was it an\n",
      "intro functional move or not so\n",
      "basically everything we could think of\n",
      "that really has an influence or so\n",
      "typical data that you also have in your\n",
      "HR System you can find data here this\n",
      "link via GitHub repository and there's\n",
      "also a PDF attached that gives you a\n",
      "really detailed description of each of\n",
      "the fields okay so that you can see what\n",
      "is there in each field\n",
      "and what does each of these entries mean\n",
      "in case you have some questions about it\n",
      "you can of course again come back to us\n",
      "as you can see here on the right the\n",
      "Flight Risk this is from the historic\n",
      "data yeah so that you see who left the\n",
      "company actually in the last 12 months\n",
      "and this is what we will use to train it\n",
      "yeah but you will use to train it so you\n",
      "can then of course apply it to the new\n",
      "employees and see who is up next yeah\n",
      "um who has a really high churn rate and\n",
      "also maybe you can also find some\n",
      "reasons why someone is leaving which\n",
      "then could prevent yeah so which could\n",
      "then be a good tip for our managers so\n",
      "they can do something about it so that\n",
      "they will stay with us\n",
      "okay that's it from The Challenge\n",
      "like I said if you have any question you\n",
      "can ask them now in the chat or also\n",
      "later on\n",
      "um I will head over to Andreas to talk a\n",
      "bit more about the organization\n",
      "yeah super thank you uh Sarah so if we\n",
      "just stay on the slides Kristoff if we\n",
      "just uh\n",
      "continuous with it that'd be perfect\n",
      "just coming up if you just go\n",
      "to the next slide please yeah\n",
      "super thanks so much so I'm sure you all\n",
      "Keen to to get started now you heard\n",
      "about the machine learning in Hana Cloud\n",
      "you've seen it you heard about the\n",
      "challenge so now to some practical\n",
      "details I'll know the next three weeks\n",
      "really now is going to continue\n",
      "and today at the time of presenting or\n",
      "recording it is in November the 28th\n",
      "and you have now three weeks to work on\n",
      "this challenge to implement at the end\n",
      "of these three weeks to present your\n",
      "results as Sarah has explained\n",
      "um so the first sort of um contact with\n",
      "a few participants already had last week\n",
      "where we offered connectivity sessions\n",
      "for people uh to connect from the local\n",
      "python environment to the Hana Cloud\n",
      "which I understand has all been working\n",
      "very well so far\n",
      "and as part of the support that we are\n",
      "providing now to help you so um bring\n",
      "the implementation we're providing these\n",
      "open Office hours that we've already\n",
      "mentioned but here we have these dates\n",
      "of these open Office hours and together\n",
      "with a link where you find the exact\n",
      "hours and due to the global nature of\n",
      "this challenge we're offering different\n",
      "time slots\n",
      "yeah the first line that you see at the\n",
      "bottom of the of the slide is for the\n",
      "European and Asia pack time zone and the\n",
      "second line further below is for the\n",
      "Americas so that everybody has the\n",
      "chance not to talk to someone during\n",
      "normal working hours of course\n",
      "and you will be supported by a global\n",
      "team of expert that we see on the next\n",
      "slide just off if you have a kind\n",
      "thanks and you see now we have experts\n",
      "in North America in South America Peru\n",
      "can Andres we have experts in in Asia\n",
      "Pacific Raymond but also the People\n",
      "based in Europe that I am called at the\n",
      "moment so we're not always available on\n",
      "all of the course calls there'll be of\n",
      "course but no there's I hope enough\n",
      "options to get in touch and discuss with\n",
      "that support\n",
      "and then essentially the assessment\n",
      "breaks down in five major steps\n",
      "that we will see here\n",
      "so first of all you need to implement\n",
      "the prerequisite prerequisite as the\n",
      "already listed listed on the blocks that\n",
      "are provided\n",
      "essentially setting up your python\n",
      "environment I'm installing the San ml\n",
      "library that allows you to connect to a\n",
      "Hana system\n",
      "and then once you have this set up you\n",
      "you connect you test that connection\n",
      "that this is working well and once you\n",
      "know you have a connectivity you may\n",
      "want to get some practice and here we\n",
      "provide tutorials with different data\n",
      "sets that gives you guidance that gives\n",
      "you code to get started to experiment\n",
      "and once you feel sort of comfortable to\n",
      "try out you know your skills on this\n",
      "data set now on the churn scenario this\n",
      "would be number four and most likely\n",
      "this is of course where you spend most\n",
      "of the time and there's no right there's\n",
      "no wrong in data science there's a lot\n",
      "of going back and forth where it Bridges\n",
      "so you might um experience that as well\n",
      "in this scenario and eventually you know\n",
      "we are looking extremely forward to what\n",
      "you're presenting what you created so\n",
      "this will be very nice and um at the end\n",
      "of the next three weeks\n",
      "and finally I'm a little bit more detail\n",
      "on the next slide thank you Kristoff you\n",
      "know where it's important to point out\n",
      "that the free trial that you always have\n",
      "access to with Hana cloud and data\n",
      "warehouse Cloud but these are not\n",
      "sufficient to use the machine learning\n",
      "this is simply not enabled in the free\n",
      "trials\n",
      "they exist in the core product and some\n",
      "participants already have for example\n",
      "their own data Mouse cloud system that\n",
      "they're using so um be sure to use the\n",
      "credentials that we're providing\n",
      "specifically now for this community\n",
      "challenge if you don't have your own\n",
      "system\n",
      "many of you have already received these\n",
      "you just need to reach out by email so\n",
      "that we can respond with the logon\n",
      "credentials if you haven't received them\n",
      "yet\n",
      "here is the email address please send us\n",
      "a note and very quickly in Return of\n",
      "course you get these credentials and on\n",
      "that slide we're also listing again the\n",
      "block with the exact powers for these\n",
      "sessions that we're providing where you\n",
      "can get in touch sometimes it is easier\n",
      "to talk and we hope even if possible if\n",
      "needed we can do a little breakout\n",
      "session that we can discuss in one to\n",
      "one maybe looking at your individual\n",
      "systems but it's also absolutely fine as\n",
      "you prefer to post a question for\n",
      "example here on the on the thread that's\n",
      "also listed that anybody can to see a\n",
      "question we can respond to everyone to\n",
      "see so that everybody benefits from the\n",
      "discussions that are going on so the\n",
      "choice is completely down to you how\n",
      "you'd like to get in touch\n",
      "with that we explain the challenge and\n",
      "so many of you are ready to go\n",
      "and I headed back to Susan and maybe\n",
      "you've already had some questions in the\n",
      "Q a\n",
      "perfect so thank you Andreas and you're\n",
      "exactly right there is a lot going on in\n",
      "our q a tool\n",
      "so\n",
      "um Kristoff was already really really\n",
      "busy answering some of these questions\n",
      "but I think it's good to also have the\n",
      "answers on The Talk track as well\n",
      "so first question we should maybe answer\n",
      "is are we allowed to use local libraries\n",
      "for fitting\n",
      "so yeah maybe I can can take that or do\n",
      "you want to go\n",
      "yeah have you detect\n",
      "Ive you know so basically it's a Hana\n",
      "machine learning challenge we might be a\n",
      "little biased towards using the engines\n",
      "now\n",
      "um in Hana Cloud obviously so this is\n",
      "really the purpose\n",
      "um so I think for that challenge so uh\n",
      "it clearly it would be nice to use the\n",
      "engines provided inside time outside the\n",
      "gel\n",
      "perfect yeah so the next question\n",
      "closely related to the first one is can\n",
      "I also use sky keflow and Pipelines\n",
      "yeah I think that would be the same\n",
      "answer\n",
      "yes so you're here we're really biased\n",
      "towards Hana Cloud so um this is a\n",
      "natural Challenge and we would really\n",
      "really appreciate if you could also use\n",
      "how I call to solve our challenge\n",
      "so next question in the Q a tool I'm\n",
      "just scanning is can you share the\n",
      "notebook from the demo so Yannick this\n",
      "one would go to you I guess\n",
      "of course yep so we'll\n",
      "um have the blog post for for the demo\n",
      "we showed and also um The Notebook is\n",
      "available in a GitHub repository which\n",
      "we would share yeah\n",
      "perfect yes so for your information the\n",
      "GitHub repositories also already posted\n",
      "to the chat so you can just click the\n",
      "link out of our YouTube chat and open it\n",
      "up so you also get our repository that\n",
      "was used for the demo let me scan so\n",
      "we're having one other question or there\n",
      "are also coming more questions and I can\n",
      "see\n",
      "um it is regarding the data set so um\n",
      "anus is asking is this anonymized really\n",
      "like data where it is possible to get\n",
      "some real insights or random data and I\n",
      "remember we talked a lot about the data\n",
      "set so maybe you want to comment on this\n",
      "one\n",
      "um yeah so of course\n",
      "um for for this community challenge we\n",
      "couldn't give out\n",
      "um real data\n",
      "um especially in for such employee\n",
      "Journal use cases so\n",
      "um it's simulated data but of course we\n",
      "made sure to\n",
      "make it quite tricky and um that it's\n",
      "also very close to the reality of such a\n",
      "use case\n",
      "perfect so um please for everyone who's\n",
      "still with us in the call if your\n",
      "question is answered and you're still\n",
      "having like open points please let us\n",
      "know and we're happy to to once again\n",
      "answer questions if there are still\n",
      "still things open\n",
      "so\n",
      "um next question we will definitely post\n",
      "uh the link for yannick's blog in the\n",
      "chat in just a second and then there is\n",
      "one last open question\n",
      "from Sergey\n",
      "um there is no ranking submission with\n",
      "unseen data is that correct\n",
      "I'll take that yeah so that is that is\n",
      "correct yes\n",
      "um because here the data that we're\n",
      "working with as the senegade bank now is\n",
      "not realistic data it's realistic but\n",
      "with real data and for that reason we\n",
      "thought it's uh it's very tricky you\n",
      "know to then test the model on a true\n",
      "hold out sample so we are probably more\n",
      "looking for creativity in the way you\n",
      "approach it\n",
      "perfect so thank you Andreas um Sega let\n",
      "us know if this answers your question\n",
      "but I think this was a really crisp and\n",
      "precise answer so we should be good here\n",
      "are there any other questions open I\n",
      "hope I have not overlooked them in the\n",
      "chat as this one is really crowded\n",
      "so last call for any questions\n",
      "okay so it looks a little quieter now\n",
      "but\n",
      "yep perfect so say this question was\n",
      "answered so I think we can close it off\n",
      "a little earlier\n",
      "um that was a great presentation thank\n",
      "you so much for sharing your challenge\n",
      "and I think I can speak for the whole\n",
      "team\n",
      "we are really really excited um to kick\n",
      "the challenge finally off and to see a\n",
      "lot of great results oh sorry I just got\n",
      "one question in the chat so let's come\n",
      "back to to answering this one so\n",
      "Enriquez asking what is preferable to\n",
      "use Sac or Hana Cloud for doing\n",
      "Predictive Analytics\n",
      "for this one\n",
      "um it depends basically on your skills\n",
      "so with sap analytics Cloud actually\n",
      "uses the same mathematics in the\n",
      "background as the automated predictive\n",
      "Library so if you're more of a business\n",
      "user and you're not so familiar with\n",
      "like Yannick showed you python coding or\n",
      "R coding and then you feel probably more\n",
      "comfortable with using sap analytics\n",
      "Cloud because there in the function\n",
      "which is called smart predict you can\n",
      "just with a few clicks also use the\n",
      "automated predictive Library without any\n",
      "code\n",
      "and but of course you're a bit Limited\n",
      "in sap analytics Cloud you could just\n",
      "use the automated predictive Library if\n",
      "you want to have more flexibility use\n",
      "different algorithms like Yannick showed\n",
      "you a random forest and then you will\n",
      "probably use the predictive analysis\n",
      "library and then you definitely need to\n",
      "go to the Hana machine learning part and\n",
      "sap Hana cloud\n",
      "I hope that helps you\n",
      "perfect so thank you Sarah for answering\n",
      "this and I think this was a really\n",
      "comprehensive summary\n",
      "so let me close it off once again\n",
      "um thanks for joining us today\n",
      "um we're excited to see a lot of um\n",
      "challenge submissions and as Andreas\n",
      "broadly explained we're here for you so\n",
      "if you're running into any issues\n",
      "problems troubles whatever please feel\n",
      "free to reach out contact us join us for\n",
      "the open Office hours comment on our\n",
      "blog posts we really want to tackle the\n",
      "challenge together with you yeah so\n",
      "thanks and uh talk to you maybe in one\n",
      "of our open Office hours\n",
      "bye bye\n",
      "hi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ytb_doc = loader.load_data(ytlinks=[ytb_link])\n",
    "ytb_content = ytb_doc[0].text\n",
    "print(ytb_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6b4c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_ytb_hana_ml_call_20230126 = loader.load_data(ytlinks=[ytb_hana_ml_call_20230126])\n",
    "# doc_ytb_hana_ml_call_20230126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2375e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def time_now():\n",
    "    now = datetime.datetime.now()\n",
    "    formatted = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "#     print(formatted)\n",
    "\n",
    "time_now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7578ebdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama_ytb_hana_ml_call_20221128'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'llama_ytb_hana_ml_call_20221128\\\\data'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'llama_ytb_hana_ml_call_20221128\\\\storage'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'llama_ytb_hana_ml_call_20221128\\\\source'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# path_llama = \"llama_mvp\"\n",
    "path_llama = 'llama' + '_' + ytb_name\n",
    "path_from = path_llama + \"\\\\source\"\n",
    "lct = llama_context(path=path_llama)\n",
    "\n",
    "display(lct.path)\n",
    "display(lct.data_dir)\n",
    "display(lct.perisit_dir)\n",
    "display(path_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63cde310",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_from):\n",
    "    os.makedirs(path_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30a3fb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama_ytb_hana_ml_call_20221128\\\\source\\\\ytb_hana_ml_call_20221128.txt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = ytb_name + '.txt'\n",
    "ytb_file = os.path.join(path_from, filename)\n",
    "ytb_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "655d7f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ytb_file, \"w\") as file:\n",
    "    file.write(ytb_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "148d2c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "llama_ytb_hana_ml_call_20221128\\data deleted successfully!\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Delete data directory\n",
    "time_now()\n",
    "run_create_save = True\n",
    "if run_create_save:\n",
    "    lct.del_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "717c21a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Files: 1 copied to folder: llama_ytb_hana_ml_call_20221128\\data!\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "time_now()\n",
    "# Copy files from source to data directory\n",
    "run_create_save = True\n",
    "if run_create_save:\n",
    "#     path_from = \"llama_mvp/source\"\n",
    "    lct.copy_path_from_to_data_dir(path_from) # default extension *.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e4345e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['path', 'perisit_dir', 'data_dir', 'data_dir_counter', 'cost_model_ada', 'cost_model_davinci', 'price_ada_1k_tokens', 'price_davinci_1k_tokens'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(lct).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd822712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Documents loaded: 1.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "time_now()\n",
    "# Load documents\n",
    "run_create_save = True\n",
    "if run_create_save:\n",
    "    lct.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bee30ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Total tokens: 8490\n",
      "Total estimated costs with model ada: $0.003396\n",
      "Total estimated costs with model davinci: $0.2547\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "time_now()\n",
    "# Estimate costs\n",
    "run_create_save = True\n",
    "if run_create_save:\n",
    "    lct.estimate_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f6ad1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "GPTVectorStoreIndex complete.\n"
     ]
    }
   ],
   "source": [
    "# https://platform.openai.com/account/api-keys\n",
    "%time\n",
    "time_now()\n",
    "# Vector create does embedding and costs tokens\n",
    "run_create_save = True\n",
    "if run_create_save:\n",
    "    lct.create_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb6d9c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Index saved in path llama_ytb_hana_ml_call_20221128\\storage.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "time_now()\n",
    "# Save index\n",
    "run_create_save = True\n",
    "if run_create_save:\n",
    "    lct.save_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5472eff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "time_now()\n",
    "# Method load_index() costs as method create_vector_store() but you don't need to upload data\n",
    "run_load = True\n",
    "if run_load:\n",
    "    lct.load_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "952ee2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(lct.index.vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f078570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(lct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cb5b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(lct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ece1cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lct.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a1cb851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Query_engine started.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "time_now()\n",
    "# Start query engine\n",
    "lct.start_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f5b399a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lct.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee02e6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1947 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The content is about a challenge to use SAP Hana Machine Learning to predict employee churn within the next 12 months. The challenge includes using the automated predictive library or the predictive analysis library available on the Hana Cloud. Participants are also encouraged to create an appealing presentation with a clear message and to consider the business context and a Persona in mind for whom they are building the solution.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "time_now()\n",
    "question = \"What is content about?\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57ba3c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Anyone who is interested in participating in the Employee Churn Challenge can participate. The challenge is open to people from all over the world, and the global team of experts is available to provide support. Participants must have access to the Hana Cloud and Data Warehouse Cloud, either through a free trial or their own system.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who can participate?\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7dc2d62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The organizers of this challenge are Sarah, Kristoff, Raymond, Andres, and Andreas.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who are the organizers?\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f676d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-Hana Cloud\n",
      "-Predictive Analysis Library\n",
      "-Random Forest\n",
      "-Hyperparameter Tuning\n",
      "-SQL Scripts\n",
      "-Data Preparation\n",
      "-Machine Learning\n",
      "-Training Set\n",
      "-Testing Set\n",
      "-Confusion Matrix\n",
      "-Random Forest Classifier\n",
      "-Predictions\n",
      "-Variables\n",
      "-Model Storage\n",
      "-Automated Predictive Library\n"
     ]
    }
   ],
   "source": [
    "question = \"Extract all technical terms.\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "454b733f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hana Cloud, Hana system, Hana Cloud and Data Warehouse Cloud, Hana Machine Learning, SAP Hana Cloud\n"
     ]
    }
   ],
   "source": [
    "question = \"Extract all unique HANA terms. Do not repeat terms.\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ecc925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
