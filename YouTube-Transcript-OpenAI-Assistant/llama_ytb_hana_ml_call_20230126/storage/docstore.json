{"docstore/metadata": {"deada3d5-c518-4a3e-97b7-25e9c7c6ff44": {"doc_hash": "d98eb48cc0d2035ed47ee50da26d4177648a8bb9fbcd93b14d452fca26592756"}, "ef9169da-6b0a-4c32-9d93-9e1753565348": {"doc_hash": "bc179510f345a013d502f6368e422e00e5963777cfa9d3ed89fb847fa424c888", "ref_doc_id": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44"}, "53522092-dbec-426e-a779-76ba974d2769": {"doc_hash": "5fd90f58c947368c5e37842a4075ce14c7be11eff48e4e91d271566a38fdbca7", "ref_doc_id": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44"}, "45800f4a-e312-4f64-9e93-4b7ffaf72aea": {"doc_hash": "3ca9095c0d7ffb2469965b5fff6b5be34c2d14db2e8a9192bfb63a1cd018acaf", "ref_doc_id": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44"}, "b5c9b7cc-cd21-441c-9ef4-8fdcd9b8c07a": {"doc_hash": "f2efb4dc390a6cda24d9176b2b0b3931483f3c591f0febdf47939f1c24d4cc03", "ref_doc_id": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44"}, "7d088e76-4cd8-468f-b196-1effbab799f2": {"doc_hash": "ea2e84b61eb83b86a2cefd3092bccbcfff72b97e942c7131036c2810c5920baf", "ref_doc_id": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44"}, "c7a0b19c-68c0-4301-85c1-e17a638446e5": {"doc_hash": "3a56914d0a7a2ab8c22804b09454cb6624eb2414f65ba645961062b412c4b8c9", "ref_doc_id": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44"}, "3735dcec-96a3-4c2d-97c5-c50784a8587c": {"doc_hash": "d556e81d9f41bfacf85df19891bd43ce45d2d4a9dd9920a2e4446c02665bb0a5", "ref_doc_id": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44"}, "4142bedf-19a9-4931-aa14-ba468af9ec83": {"doc_hash": "7d8c546e751416ce031af9a56c0e8c662454ec17c40ffa57205579dba98a36fe", "ref_doc_id": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44"}, "50bdd7e6-2293-4b23-943f-e3ace9abd902": {"doc_hash": "5adc9a3953b7893c569247421c192d4f10e692851b02456d60756150b3deba45", "ref_doc_id": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44"}, "cd5e6ece-3314-4112-b414-896f3d7678f1": {"doc_hash": "87cac896e943dc9f4541fff05851f609ea704be63be39c810d934ee6ba65321b", "ref_doc_id": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44"}, "35faf7a1-2577-4d2d-b4bc-8a41ec790a5a": {"doc_hash": "371b0ed272ccb808de30c75226682bf78516ec9f7c820538bbc013ec2d743af6", "ref_doc_id": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44"}}, "docstore/data": {"ef9169da-6b0a-4c32-9d93-9e1753565348": {"__data__": {"text": "it\nwelcome everyone\nWelcome to our sap Community call today\nI'm glad to see so many of you here in\nour call\num very special sap Community call it's\na winner announcement of our sap Hana\nCloud machine learning Challenge and I'm\nmore than excited to be joined here\ntoday by our three winners of our\nchallenge\num I'm Lena from The SFU Community team\nand we're hosting this call today with\nthe sap Hana Cloud team\nso um let me introduce the winners to\nyou\num it's vishwas madhubashi Sergio yatko\nand Clements Vera\nwho made it here on our virtual stage\nand won the challenge and our today able\nto present to you each for 15 minutes\nhow they answered the main question\nnamely how to prevent employee churn by\nusing sap Hana Cloud's machine learning\ncapabilities\nthe challenge ran for three weeks in the\nsap Community back in November until\nmid-December and we had quite\ninteresting solution paths but you three\nmade it so congratulations here\nofficially to every one of you\num before we start and before I head\nover to\num Krista from our sap Hana Cloud expert\nteam to give you a little bit more\nbackground on what the challenge really\nwas about\num let me briefly emphasize on um the\nfact that we have time after\num vishwas selju and Clements presented\nto all of you\num to address any questions that might\ncome up\num during the presentations or and that\nyou have in general with regards to sap\nHana Cloud machine learning so we have\n10 to 15 minutes towards the end of this\ncall to address these and therefore\nplease log into YouTube here\num and type the questions in the chat\nI'm already also let us know where we're\ntuning in from how are you feeling today\nand what's on your mind so let's have a\nlively chat here today in our YouTube\ncall\nand with that being said I'll hand over\nto Crystal to share his screen\num to give a little bit more background\nbefore then uh which of us will start\nwith his first presentation\nthank you so much Lena for the warm\nintroduction and welcome I just wanted\nto quickly uh uh recap what the\nchallenge was about uh how to predict\nemployee churn so we provided a data set\nabout employee data with different\nfeatures and certainly kind of an\nindicator that people uh kind of left\nthe company Flight Risk was the what's\nthe column name and then the challenge\nwas for the participants of the\nchallenge we had about 20 people kind of\nactively participating and running their\nmachine learning scenarios using the\nHana Cloud machine learning libraries\nand capabilities to come up with a\nprediction model which would then help\nthe HR department to yeah in the end\nprevent employee churn so the idea was\nkind of what can we find out in the data\nand guide and help the HR team to\nprevent\nemployees in the different sections of\nthe company to not leave the company so\nfor everybody who couldn't participate\nin the actual challenge there are a\ncouple of links here which will also\nshare and which you will certainly also\nfind searching for them the initial\nblock uh we have a sample repository\nwhere we provided the data so it's\navailable for you to also kind of pick\nup the challenge even now or later in\nour uh Hanam samples repository we had\nlots of great questions during the\ncommunity challenge in our community\nthread I've also shared the link here\nand certainly then we have the the\nwinners plot which Susan our colleague\ncreated and\nreally now with no further Ado we want\nto kind of learn uh from our winners\num how they approach The Challenge and\nwhat were their solutions to the\nchallenge\nand with that I hand over to vishwas for\nuh his first solution presentation\nthank you Kristoff so let me start\nsharing my screen and talk through the\nsolution\ncan you see my screen now\nyes we can\nokay\nso the the challenge was to predict the\nemployee churn and for that we had some\ndata and some uh some very very very\nclear uh objective let's see what the\ndata was and what the objective was\nso\nthe\ncontext\nthe context was that the companies want\nto", "doc_id": "ef9169da-6b0a-4c32-9d93-9e1753565348", "embedding": null, "doc_hash": "bc179510f345a013d502f6368e422e00e5963777cfa9d3ed89fb847fa424c888", "extra_info": null, "node_info": {"start": 0, "end": 3963, "_node_type": "1"}, "relationships": {"1": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44", "3": "53522092-dbec-426e-a779-76ba974d2769"}}, "__type__": "1"}, "53522092-dbec-426e-a779-76ba974d2769": {"__data__": {"text": "objective was\nso\nthe\ncontext\nthe context was that the companies want\nto retain their High performing\nemployees and the to reduce the churn\nrate of those employees and right now\nthe managers uh try to think about who\ncan leave and they they try to prevent\nprevent the addition of those employees\nbut this is very subjective not based\nupon data and can be wrong most of the\ntimes so the intent was to\ntake a look at the concrete data and\nanalyze the patterns of the employee's\nchurn and figure out which are the\nemployees who are highly likely to at\nright and stop them from leaving the\ncompany if they are valuable to the\ncompany\nmanagers will continue using their\nintuition for preventing the employee\nchurn but the intent of the challenge\nwas to figure out which models can be\ndeveloped to predict the churn of the\nemployees at certain events certain\ncertain times certain areas and make\nsure that we can take the necessary\nsteps before the chair becomes obvious\nbefore they put in their paperwork and\nuse mathematics and data to arrive at\nthe conclusion that we can rely upon\nfor this objective we were given about\na big set of data we had about 90 000\nsamples uh with the 42 independent\nvariables and we had one dependent\nvariable called the Flight Risk out of\nthese samples we were able to figure out\na few obvious groups and we have marked\nthem in on the screen right now one\ngroup was about the age we had age age\ngroup 10 age group 5 and generation so\nwe had we were able to reduce some of\nthis data based upon uh finding by\nfinding the most obvious one most useful\none then we had a second group for the\nprevious functional area previous job\nlevel all those things from the previous\nprevious uh previous job uh then we have\none group about the that location we had\ncurrent country latitude longitude\nregions we were able to drop some points\nfrom here as well and finally we had the\nscientist as the fourth uh fourth Point\nhere which was the dependent variable\nthe data was synthetically generated but\nit was supposed to mimika real life\nscenario\nuh the Eda insights for this uh for this\nwere our pretty interesting\nwhat we found that 8 out of the 42\nindependent variables have equal number\nof missing values which meant that these\nuh there was something very common in\nthese variables and you were able to\nfigure that they were they had all the\nuh all the same requires for had all the\nsame missing values we were able to drop\nsome of the records to make sure that\nyou are not counting uh the records of\nthe missing data and we were not able to\ngenerate those because they meant they\nwere related to the previous jobs since\nwe could not uh create the editor we had\nto drop some records\none uh one interesting uh and kind of uh\ndisturbing thing was we were able we we\nfound that there was some potential\nanomaly in the data\nuh we saw that the 71.5 rules had been\nmore critical\nand by 28 were marked as non-critical so\nthis seemed like an annually to us and\nwe have uh pointed this out to sap and\nuh hopefully uh with the correction of\ndata we can arrive at the better models\nbut this was one of the obvious outliers\nthat we saw in the data itself\ncorrelations were not there but not many\nwe could not we don't did not find any\nsignificant correlation in the\nmathematical columns the only two small\ncorrelations we found were in the uh\ntime time in previous previous question\nmonth and promotion within last three\nyears but the correlations were very\nminimal uh just about\n0.15.17 not too high so these\ncorrelations were not really significant\nso they did not take away anything from\nthis part\none thing that we found was that the age\nand job type were kind of correlated\nwith the attrition as the age increased\nthe attrition started dropping\nin the 40 50 in the 55 plus group the\naddition rate virtually drops to zero\nwhile it is highest in the 20 to 30\ngroup there was one outlier though uh\nthe 45 250 group but again in general\nthe increase in age indicated the\nadmission dates rates kept going down\nwith that\nuh the other was that the temporary\nemployees were more likely likely to\nattract and there is no surprise uh so\nthis was obvious", "doc_id": "53522092-dbec-426e-a779-76ba974d2769", "embedding": null, "doc_hash": "5fd90f58c947368c5e37842a4075ce14c7be11eff48e4e91d271566a38fdbca7", "extra_info": null, "node_info": {"start": 3900, "end": 8047, "_node_type": "1"}, "relationships": {"1": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44", "2": "ef9169da-6b0a-4c32-9d93-9e1753565348", "3": "45800f4a-e312-4f64-9e93-4b7ffaf72aea"}}, "__type__": "1"}, "45800f4a-e312-4f64-9e93-4b7ffaf72aea": {"__data__": {"text": "more likely likely to\nattract and there is no surprise uh so\nthis was obvious and this is visible in\nthe uh in the uh in the graph at the\nbottom as well\nfunction and region were also a small\nscale indicator of the attrition we saw\nthat the people in sales and marketing\nwere at a higher risk of its writing and\nthat's what we have marked in the uh in\nthe uh in the graph above let me just\nhighlight that as well\nit's this part we also saw that the apj\nregion had the highest attrition rate\nnearly double than than other regions so\nI think there may be something going on\nin the APG region so I think the apj\nregion and these sales and marketing\ncutting departments had something going\non that was contributing to the highest\nhigher education rates so this is one\narea that we should take a look at\nwe also found that the decreasing\nperformance rating was a good indicator\nof attrition so no surprise here and\nalso a demotion was a good indicator of\nthe of the higher iteration so no\nsurprise here the the ID is do we even\nwant to stop these employees if somebody\nhas a decreasing performance do you want\nto stop those employees I think we have\nto talk about the HR to take a look at\nthe next next action for these employees\nor pre-processing we filled all the nand\nvalues with an assigned and then we\nsplit the data and data into 10 desktop\nvalidation in the 80 10 10 ratio and\nthen we did all the uh all the\npreprocessing and uh and we went ahead\nwith the model building\nwe found that the sap made the life of a\ndata scientist very easy\nhad we not had all the functions\navailable within sap we had to go\nthrough all these\nall these steps dropping 12 columns\nincluding pen columns mapping zero mean\nfor the binary values getting dummies so\nall those things were kind of not\nrequired with sap so this was I think a\nlot of times it happened to us because\nwe were using Sap's machine learning\nmachine learning solution\nlet's talk and take a look at the model\ninsights\nso for the first model we use the\ngradient boosting binary classifier\nand with this one we were able to and\nagain since we are trying to create the\npredict the churn we are looking at\nrecall as the the value of interest uh\nwith this model the recall value that\nyou got was a low value just about 55\nand one percent not really encouraging\naccuracy was good decision was not not\nreally uh encouraging either an FN score\nwas low AUC boss it was decent but again\nnot really uh but this model overall was\nnot not really encouraging\nuh one thing that we point out here is\nthat critical job role was low\ncontributory here and again this is one\nmore thing that I have pointed out\nearlier that this is one area that may\nhave uh impacted our models and they\nshould take a second look at this one\nalso one part is that salary was about I\nthink ninth or 10th in the hierarchy so\nfunny enough salary has not featured as\nthe top performing in any of the models\nso salary is not the core concern of\nemployers who are leaving the job uh\nthere are other factors we have to take\na look at those more uh more\ninterestingly\nsince model 1 was not really uh\nimpressive we went on uh to build two\nmore models so for the model 2 we used\nthe random forest classifier and in this\nmodel we were able to achieve a recall\nvalue of nearly 94 percent so this was\nbetter than the previous one accuracy\nwas also reasonably good reference code\nwas was decent diesel was Precision was\nnot really high but again given that\nrecall is very high prison was supposed\nto go uh low up a bit which is to be\nexpected in this also the salary is\nfar below so I think we can safely say\nthat salary is usually not the highest\nreason why the employee script there are\nvarious other things that we should\nshould take a look at facility is not\none of those and I don't see the uh\nbe uh a critical role here in this at\nall so I think critical role has has\ndone something uh the data in critical\nhas done something to these models and\nwe should uh we should make sure to to\nsee uh that we have the right values for\nthe critical role for all the", "doc_id": "45800f4a-e312-4f64-9e93-4b7ffaf72aea", "embedding": null, "doc_hash": "3ca9095c0d7ffb2469965b5fff6b5be34c2d14db2e8a9192bfb63a1cd018acaf", "extra_info": null, "node_info": {"start": 8047, "end": 12103, "_node_type": "1"}, "relationships": {"1": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44", "2": "53522092-dbec-426e-a779-76ba974d2769", "3": "b5c9b7cc-cd21-441c-9ef4-8fdcd9b8c07a"}}, "__type__": "1"}, "b5c9b7cc-cd21-441c-9ef4-8fdcd9b8c07a": {"__data__": {"text": "sure to to\nsee uh that we have the right values for\nthe critical role for all the employees\nso we also build one more model to\nensure that we are on the right track so\nfor the model 3 we use the hybrid\ncreation boost gradient boosting three\nand with this one we are able to achieve\nat lower recall value of 56.25 not\nreally not really uh encouraging\naccuracy was uh just about six to six\nplease signals are very low F1 was very\nlow so this is not really a good model\nand somehow the ROC curve uh stooped\nhere in between this was supposed to be\nsmooth here but this two tier for some\nreason I we have then we have to do some\nmore more research in this area uh but\nagain this model was not also performing\nat the level expected so we did not uh\nthink that this was a great model to\nmove ahead with in this also salary was\npretty low uh so the syllabus number\nseven here I guess uh but also the uh\nI don't even see\nthe critical role is not showing up here\nit's very low so so salary is very low\nin this in the rating and critical job\nrule is also fairly low in this area as\nwell\nfor deployment part uh we believe that\nonce we choose the right model we have\nto look at several factors that can lead\nto uh leap to employ electrician and I\nhave listed 10 factors here which came\nfrom a vented Circle so these 10 factors\nI'm pretty much sure that the HR is\nlooking at these factors and making sure\nthat these are taken care of\nbut the intent was to ensure that the\nbest employees stay with us we have to\nmake sure that we run this model on\ncertain critical events\nand I have listed some events here uh\nthe annual appraisal is one critical\nevent promotion is a is a critical event\nfunction slash Department change is also\na very critical event\nuh an employee's birthday as the\nemployees age their attrition Tendencies\ngo down\nany major life event for an employee\nmarriage separation birthday so these\nare all the events where the employees\nare likely to make uh changes to their\nintent to it right and the idea is that\nthe HR should run this model for these\nemployees at these critical events and\nensure what has what has changed and\nbased upon that they should take some\naction what he promote is that we should\ngenerate a we should run these reports\nand Report weekly and the HR should get\nthis report by a in a pushed model\nnotable model so they should get the\nreport and based on that report they\nshould Define the actions for the next\nfew months few weeks few days few years\nbased upon what has happened and uh\nbased upon that they should figure out\nwhat they have to do to ensure that the\nemployees who are highly likelihood\nright and that they want to remained\nwant to retain uh are given what they\nwhat they what they need to stay in the\ncompany\ntomorrow should not be used in must it\nshould be they should be you know uh\nvery critical to practical to the\nemployees so it should be written only\nfor those employees which have which\nhave seen any major event in the last\nweek or so and the HR should get the\nreport for only those employees and take\naction based upon that report\nour conclusion is that the model to the\nrandom Forest classifier is the one that\nhas given us the best performance about\n94 recall value and it makes sense\n[Music]\nforeign\nwe should try to get more data and make\nbetter model based upon that\nuh we must ensure to have a feedbook\nfeedback loop uh to ensure that whatever\nyou learn is passed back to the model\nand the model keeps improving with every\niteration of the Run of this model\nuh as you mentioned the sales and\nmarketing departments and the APG region\nhad some tendency uh has shown the for\ntendency for people to attract more so\nwe should see what is happening in these\nregions and this and these departments\nto ensure that we can uh we can plug the\nGap and ensure that the best employees\nstay with us\nuh salary is not among top five\ncontributors so we we can test easy that\nsalary is using what the reason why\nemployees employees leave and we should\nfocus on the other factors\nand finally we should discuss with HR\nwhether", "doc_id": "b5c9b7cc-cd21-441c-9ef4-8fdcd9b8c07a", "embedding": null, "doc_hash": "f2efb4dc390a6cda24d9176b2b0b3931483f3c591f0febdf47939f1c24d4cc03", "extra_info": null, "node_info": {"start": 12103, "end": 16161, "_node_type": "1"}, "relationships": {"1": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44", "2": "45800f4a-e312-4f64-9e93-4b7ffaf72aea", "3": "7d088e76-4cd8-468f-b196-1effbab799f2"}}, "__type__": "1"}, "7d088e76-4cd8-468f-b196-1effbab799f2": {"__data__": {"text": "should\nfocus on the other factors\nand finally we should discuss with HR\nwhether we should filter out the\npredictions of Flight Risk for the for\nthe uh employees who are who have a\ndecreasing performance rating and who\nhave been demoted do we want to even\nretain them we should talk to HR to\nensure that we can take the right action\nand not burden HR with the uh with a\nfull report of all the employees uh even\nincluding those employees that we that\nthe company does not retain so these are\nthese are the findings that we think are\nare useful for the HR and with this I\nconclude my presentation and hand over\nto\nKristoff\nthank you vishwas\nokay I'll unmute\nI would share my screen\nokay\ncan you see my screen yes\nyes yeah\nit's okay\nyeah yeah well\nthank you thank you\num well after\num The Challenge I published the entire\nJupiter Notebook on GitHub and\nafterwards in a few weeks in January I\nalso decided to publish a Blog where I\nexplained the main parts from\nfrom the Jupiter notebook so I'll try to\ndo my best to explain in my next 15\nminutes\nokay so we already know that this is\nabout\nin about uh\nwe have we already know that we have to\npredict the employee Chan as already we\nhave mentioned\nand that\nI the first step is to install the\num Anaconda and create my\nmy environment and I also installed my\nHana ml what I like about Hana machine\nlearning is that I can access it from my\nlocal Jupiter notebook and I can write\nthe code in Python and this in Python\neverything is an objects and the Hana\nmachine learning objects are just like\nany other objects and I can wrap them in\nfunction I can use them in lists and\ndictionary and get a very clean and\nreusable code\nso understanding the challenge\nlooking into the data I can see that our\ndata is highly unbalanced imbalanced we\nhave two classes the first class is no\nclass this is a majority class and the\nnext class is the yes class the class we\nhave to predict\nand this class is our minority class\nwell the next step is to look into data\nand look into feature distribution\nI also feel the the missing values did\nsome feature engineering by creating new\nfeatures\nso I'm ready to go to my next step and\nsee what I can get from this data so I\ncan build a model to do so I have to\nsplit data into train data and test data\nI used only one function only one\nLibrary which I wrote in a function I\nused only hybrid gradient boosting three\nso this is a function where I wrapped\neverything\nafter\nI fit the model I can look into my\nstatistics well looking just into model\nstatistics\nuh\nI can consider myself lucky if I'll take\ninto consideration on the accuracy\nbecause I have an accuracy of\n0.92 but as\nbut as I but as I mentioned above I know\nthat my classes are highly imbalanced so\nI cannot\nrely on accuracy I have to rely only on\nrecall and for that\nI explained I created a new chapter\nunderstanding statistics and I explained\nwhy in imbalanced classes we have to\nrely only on recall and what should we\ndo next to reply also on\naccuracy so this is a very detailed\nexample one\nrecall is about to know and three yes so\num\nI don't know I will not go into details\nhere because it will take some time but\nwe can look into the blog afterwards if\nyou have time\nso after also I\ndid my prediction I output the\nprediction statistics and looking into\nprediction statistics I also can see\nit's wrong\nsomething wrong\nokay looking into prediction statistics\nI can see that my recall is zero three\npoint zero three four\nand that's very poor\nI will store all these initial results\nin\na local data frame so I can look into\nall my results at the end\nso meanwhile we received additional data\nand with this additional data\nwith this additional data\nuh\nI built again the model I joined all\nthis data and looking into the call I\ncan see a very good Improvement the\nrecall is jumping into zero six into 0.6\nand also looking into prediction\nstatistics I can see that my recall is\nzero point\nfive seven it's slightly under the model\nstatistic but", "doc_id": "7d088e76-4cd8-468f-b196-1effbab799f2", "embedding": null, "doc_hash": "ea2e84b61eb83b86a2cefd3092bccbcfff72b97e942c7131036c2810c5920baf", "extra_info": null, "node_info": {"start": 16159, "end": 20123, "_node_type": "1"}, "relationships": {"1": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44", "2": "b5c9b7cc-cd21-441c-9ef4-8fdcd9b8c07a", "3": "c7a0b19c-68c0-4301-85c1-e17a638446e5"}}, "__type__": "1"}, "c7a0b19c-68c0-4301-85c1-e17a638446e5": {"__data__": {"text": "recall is\nzero point\nfive seven it's slightly under the model\nstatistic but this is good\nthe next step is to reducing the number\nof features to do that I have grouped\nthe features by I have grouped features\ninto\nI have grouped dependent features\nyou can see the the groups and from each\ngroup I selected only one feature by\nimportance\nalso I did some\nadjustments for instance for previous\ncountries for uh for I selected in case\nof context I prefer to adjust the\nselection with the previous countries\nand current countries\nsomething is going in my mouse yeah and\nlooking at prediction with the\nlooking at the prediction statistics I\ncan see that my recall for yes is 0.62\nwell\nprobably think we received even more\ndata now we didn't but we have some uh\nvery powerful tool it's mode it's mode\nis about synthetic minority over\nsampling technique\nI will do one more time the I will check\non my time the class proportions\nI can see it here I can see that my plus\nyes is highly imbalanced and I have to\ncalculate the the maximum amount for um\nfrom Ice mode so the max amount is 710\nwell I will apply this function and I\nwill get my training data\nbelow and the attaining data are\nbalanced you can see it from the output\nso I would like to look into my\nprediction statistics and here we have\nthe big surprise Mighty call yes is 0.8\nand this is a very big Improvement of my\nsmoke so I'll keep this mode I also can\nlook into my prediction statistics and I\ncan see that my recall is\n0.79 it's almost the same so I can go to\nthe next step and this is I will call it\nthe kind of a magic stand because I will\ndo double magic I will balance my test\ndata and also I will increase my\ntraining data\nwhy I should do this balancing because I\nwould like to use also accuracy for my\nprediction for my uh prediction\nstatistics so you can see that I\nbalanced my test data and at the same\ntime I have increased my uh\ntraining data we see an increase here\nI will apply this mode to the detained\ndata I will use all these classes all\nthis function I built before I can see\nthat my training data uh\nbalanced they are equal\nwell I can look into my prediction\nstatistics and looking into my\nprediction statistics\nthe recall for yes is jumping is zero\npoint\neight two and looking at the support we\ncan see that our support is 210 for now\nand 210 for yes and that's mean that we\ncan rely on accuracy and my accuracy in\nthis case is 0 8\nseven zero point eight seven\nwell what I did above is tops mode it\nwas just the first try but I would like\nalso to to explore it more what does it\nmean I would like to\ndo it step by step by increasing or in\neach step with 100.\nand then I will choose from all these uh\nmodels I will choose the best one based\non my accuracy on my prediction accuracy\nbecause as I said above I can rely this\nmoment\nalso on my\naccuracy now\nso this is a very intensive process\neverything is done in Hana\nwell no I'm just sorry for that\nit was here\nit was about\nburning top smooth yeah it took about it\ntook about eight minutes it took about\neight minutes and at the end I can look\ninto all the models I've got the models\nhere and accuracy and I can see that\nthis motor is the 700s is the best one\nnow I can move to the next step building\ntop models\nand the Imperial top models I would like\nto add the features one by one in order\nof importance so I also created one\nfunction to wrap everything and this is\nalso a very very intensive process it\ntook about 21 minutes well you can relax\nenjoy a cup of coffee or of tea I did so\nbut the code is already executed here so\nwe are looking just into the data\nand we have a great surprise yeah\nbecause uh we we can see here that the\nbest mode with the best top\n15 features gives me an accuracy\nof 0.89\nI stored all these models and other\ndifferent dependencies in a dictionary\nand as I said before I like honey\nmachine learning libraries that\neverything is object and you can see in\nthis output that the model is just\nstored in this list along", "doc_id": "c7a0b19c-68c0-4301-85c1-e17a638446e5", "embedding": null, "doc_hash": "3a56914d0a7a2ab8c22804b09454cb6624eb2414f65ba645961062b412c4b8c9", "extra_info": null, "node_info": {"start": 20127, "end": 24095, "_node_type": "1"}, "relationships": {"1": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44", "2": "7d088e76-4cd8-468f-b196-1effbab799f2", "3": "3735dcec-96a3-4c2d-97c5-c50784a8587c"}}, "__type__": "1"}, "3735dcec-96a3-4c2d-97c5-c50784a8587c": {"__data__": {"text": "and you can see in\nthis output that the model is just\nstored in this list along with all\nothers dependencies which are local\nand\nI'll go to I have this last\num I have this table I have this data\nframe\ndata frame it was with all the results\nbut I'll switch to GitHub\nand because I can show here all the\nprogress so what did we have at the\nbeginning yes we have this initial data\nand our equal yes was only\n0.34 and then we have additional data\nand our recall increased and then I try\nto reduce the number of features and\nwe've got an increase and then I decided\nto do some manual adjustments and I've\ngot almost the same recall and in the\nnext step I applied this powerful tools\nmode and this mode you've got this big\nincrease of recall to zero eight\nwell I did the same with important\nstatistics then I decided in the next\nstep to balance my test data and you can\nsee that my support is 210 here and 210\nhere yeah\nand this at this stage at this step I\ncan\nrely also on accuracy and my accuracy is\nzero eight\n0.87\nand I will do the same with the best\nmode\nand finally I'll do the same with my top\nteachers and I've got this best result\nof 0.89\nswitching back to\nthe blog\nwell analyzing top models well the\nmachine learning is not a black box yeah\nwe can ask some questions and the the\nresponse the answers to our questions\nit's the reason quote\neach prediction I can get some\nexplanation for instance the reason\ncalled the top reason quotes for first\nprediction I think day six day for the\nnext one and training for the next one\nand so on\nI can\nask even more questions yes and for the\nsame prediction I would like to receive\neven more answers to give me top five\nrecent quotes\nand\nthey can do both\nsomething is\ntalking I got it okay\nso we've seen only data above but also\nwe can we can visualize all these\nstatistics we have plots we have a very\npowerful Library unified report we can\nsee all these importance we have seen\nbefore uh as a chart we can see\nalso we can output other\nstatistics model model statistics then\nwe can have explanation the form from\nthe prediction and we can see the\ncontribution of which features and for\nthat we can use shape League planer\nthis is a plot from shapely explainer\nalso everything I\ndescribed above about uh\nrecall and\naccuracy we can understand better with\nthe confusion Matrix this is the mode of\nuh this is a confusion Matrix for model\nwe can take a look at this also\nnormalized\nand also we can look at the prediction\nstatistics of we can look at confusion\nMatrix we can look at confusion Matrix\nof prediction and we can see that we\nhave here pulse no\n32 and through yes is\n178 and if we assume that we'll get 201\nyeah so and above you will get the same\nuh same value meaning that our test data\nhave been balanced and we can rely on\nall statistics recall accuracy precision\nF1 score and so on what about for the\nsteps well Machine learning\nuh has many steps well we have\nconnection data and then we have to\nbuild all these models and the model is\nthe brand but this brand needs the body\nso the body in our case is an\napplication and we should positive think\nabout how we can deploy these models\nas I said at the beginning my scope was\nto\nprovide the different\nalternatives to the same model so\nbecause in in deployment during the\ndeployment I would like to\nuse different models so I'll be able to\ncompare to do some kind of a b tests\nso that's actually everything about uh\nmy model\nand my results\nthank you Sergio you can hand over\nthanks you can unshare your screen okay\nand let's continue\ncan you see my screen\nyes we can okay perfect yeah so hello\neveryone\nmy name is Clement I'm from Austria I'm\na consultant at\nSTD and yeah so I'm Consulting business\nand analytics Banning\nand the STC is one of the leading sub\nservice providers in Austria and we are\nhappy\nto share our expertise inventiveness and\npassions to get the best out of their\nbusiness every day from our customers\nand we also a", "doc_id": "3735dcec-96a3-4c2d-97c5-c50784a8587c", "embedding": null, "doc_hash": "d556e81d9f41bfacf85df19891bd43ce45d2d4a9dd9920a2e4446c02665bb0a5", "extra_info": null, "node_info": {"start": 24097, "end": 28030, "_node_type": "1"}, "relationships": {"1": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44", "2": "c7a0b19c-68c0-4301-85c1-e17a638446e5", "3": "4142bedf-19a9-4931-aa14-ba468af9ec83"}}, "__type__": "1"}, "4142bedf-19a9-4931-aa14-ba468af9ec83": {"__data__": {"text": "to get the best out of their\nbusiness every day from our customers\nand we also a long-standing partner of\nsap and covering the entire sub solution\nportfolio\nso yeah and I'm also very happy that I\ngot the opportunity to\num get in this Challenge and yeah so I\nwant to share my insights about the\nchallenge\num the first thing was um that we had\ntoday that it is the aim that we want to\npredict the employees which are\num which want to quit but another aim I\nalso wanted to implement is how to do\nsomething against like what can the data\ndo that we can do against it\nand the first thing as a data scientist\nyou're doing and looking into the data\nand to get a little bit of feeling of it\nand yeah so I created some numeric\num histograms from the numerical\nvariables and also correlation Matrix so\nyou can see here but as\num which has mentioned before you can't\nfind something\num significant so it's very yeah very\nclear\num and the tender amounts with the age\nthat's very clear that it's positive\ncorrected in the salary with the age so\nyes and if you look at the numerator\nthere is the only thing I can find was\nthis salary because if you look there is\njust a huge amount of people who are\nearning that most of the 120 000. but\nthe other things are pretty much normal\ndistributed and you can't see anything\nspecial from it\nI also tried it to split it up\num that um to compare the people who've\num quitted the job and who created not\nso like from the salary or from the age\nand there was nothing\num special significantly so it was\nalmost the same and yes\nand another thing was the\nso I have to go to next slide\nso yes another thing what um was that\nthe data is very imbalanced so you can\nsee that for the company it's very good\nthat the most people are staying and\njust a small amount of employees are\nquitting but as a view as a data\nscientist it's not a really perfect\nscenario because you really want to find\nthe people who are quitting and if this\num is a minority test and it's it's\ngetting harder and the sap pile has an\nimplementation to\num\nbalance the data so it's called the\nsmall Tech well sorry and\num the small Tech is this mode that you\num\nuh doing the the data points which are\nexisting from the yes that you are not\nduplicating it but you uh\num creating synthetic data points and\natomic aggregious to to reduce the node\nclass that's what I've done and very\naggressively so I almost balance it from\nyeah one to one\nand that was uh the first approach\nbefore I got through the modeling\nand then we first started with the\nmodeling I decided for the average\ncreating boosting tree\nmodel and it's very nice that if you use\nthe sap power you can also do some\nresearch and because it's all calculated\nin in the server at Hana mil so you can\nuse the compute resources from it\nand find the best and the optimal and\nparameters I did a tweet search\nand I let it run for some time and\nit's very fast so I don't know anymore\nit's about 20 minutes 40 minutes and I\nplayed a lot around with it and the\nperfect parameters was for the model one\num 0.1 for the learning rate and 120\nestimators under a split threshold from\n0.1\nand\nthe thing is also\nthat\num yes but it's that is very nice that\nyou just like can write on the server\nand calculate it\num the first uh and here are all\nfeatures involved in this so I haven't\ndone something in future selection and\nhere all in it and I got a really nice\nposition from 97 percentage of no and\nfrom the yes uh position from 78\num 70. free almost\nso yes this is the detection\num that you can find it and here you can\nalso\num see the correlation Matrix so from\nthe yes we we got 325 right in just 121\num row\num what you also can do\num and see for the\nforeign\nyou get the confidence score and you\nalso get the reason code from the model\nand here you can see why so it's not the\nblack box you can see why um the model\nis um\ngiving it the score yes for", "doc_id": "4142bedf-19a9-4931-aa14-ba468af9ec83", "embedding": null, "doc_hash": "7d8c546e751416ce031af9a56c0e8c662454ec17c40ffa57205579dba98a36fe", "extra_info": null, "node_info": {"start": 28030, "end": 31909, "_node_type": "1"}, "relationships": {"1": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44", "2": "3735dcec-96a3-4c2d-97c5-c50784a8587c", "3": "50bdd7e6-2293-4b23-943f-e3ace9abd902"}}, "__type__": "1"}, "50bdd7e6-2293-4b23-943f-e3ace9abd902": {"__data__": {"text": "box you can see why um the model\nis um\ngiving it the score yes for example\n[Music]\nwhich are skipping uh quitting sorry\nand yes so this is the model one and the\nidea of the model 2 was to just select\nthe top 15 features which are involved\nin the model one so for example each art\nreading functional error change type six\ndays and so on\nand I have done this again with the with\nthe hardware ingredient boosting tree I\ndid a grid search the optimal parameters\nwas a little bit harder so different so\nit was like running rate 0.4 estimators\nwas the same with 120 in the split\nthreshold with 0.3\nbut the thing is\num I didn't get this good result at the\nmodel before so the Precision from now\nis quite the same with 97 percentage but\nthe Precision with yes\num it is a little bit\num weaker so uh I just got 68 percentage\nof it so the model one and the model 2 I\nwould suggest to to use the model one\nwith all the features even if we have\nsome other noise in the data because of\nother features and so on\num but uh the the the class we want to\npredict is yes and here is like we want\nto find the most\num highest Precision without we want to\nfind all the employees\num we're creating so this is my first\napproach and\num the second one is what what can we do\nnow if we can find now the employees\nwhich are greeting for greeting and then\nwhat can we do and what can we do just\nif we have the data from it\nand if we go now here what I have done\nis I use the top five features come from\nthe model\nand just looked into the detail what\nit's different why is the model\npredicting like that kind of and the\nfirst the most top feature is each art\nreading and it has an influence of 28\npercentage of the model so it's quite a\nhuge influence in this and the funnest\nthing or I don't know for the company\nit's not funny but if you look here on\nthe left side you have the um employees\nwhich are quitting and on the right side\nyou have the employees which I'm not\ncreating and if you look at the empress\num who are quitting you have a really\nbig amount who did HR training and the\nsmall amount who didn't have done the\nchart reading and if you look at the\npeople at the employees which are still\nat the company it's it's in the other\ndirection so yeah I don't know what they\ndo in the company the HR training what\nthey are teaching what they are learning\nbut yeah that's this one indicator that\nit's not really beneficial for the for\nthe company\nthe second\num top feature was the functional area\nchange type that has an influence of\nnine percentage of the model\nand I did the same so I splitted it into\ntwo groups with the flightless gask and\nFighters no and here you can see that\num one the no change is yeah\nis also a huge amount in the in the\nemployees which are still there\num but if you look at the\ncross-functional move so there's a\nsignificant uh significant difference\nbetween the Flight Risk yes and the\nFlight Risk new group so if you\n[Music]\num\nso my suggestion is to to\num use this in the company and to deploy\nthat cross-functional move to to prevent\nevery\nthen the third feature was the sick days\nand here um is the same on the left side\nyou see that the people who quit at the\nshop and on the right side were still at\nthe company and\num I also\num printed the mean from the flight with\nno\num amount at flight qcs so the green\nline is the is the mean\num from the from the sick days of the\npeople who are still at the company\nand the the thing is that people who are\nemployees which are quitting the job\nhave more sick days than the the\nincrease we're staying and for me it is\nlike I would suggest as an indicator\naround 116 days so if the employee has\nmore than 11 Civic tests and maybe yeah\nit can be that um\nthe employee will quit for example maybe\nalso if I did additional he did each\nartery\num and the fourth was the employment\ntype\nso\num the\num the data that was temporary and\nregular so temporary means that they\nhave a temporary", "doc_id": "50bdd7e6-2293-4b23-943f-e3ace9abd902", "embedding": null, "doc_hash": "5adc9a3953b7893c569247421c192d4f10e692851b02456d60756150b3deba45", "extra_info": null, "node_info": {"start": 31923, "end": 35847, "_node_type": "1"}, "relationships": {"1": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44", "2": "4142bedf-19a9-4931-aa14-ba468af9ec83", "3": "cd5e6ece-3314-4112-b414-896f3d7678f1"}}, "__type__": "1"}, "cd5e6ece-3314-4112-b414-896f3d7678f1": {"__data__": {"text": "data that was temporary and\nregular so temporary means that they\nhave a temporary contract and the other\none is just a normal country so the\ntemporary is limited to a date and\npeople employees they are greeting their\njobs more when they have a temporary\nEmployment contract and looking for it\nso it's maybe it's logical because they\nare looking\num for another job before the contract\nexpires and then they are switching\ncompanies or that can be but it's also\nthey are not happy maybe with it because\nwe can see it in the data that there is\na different\nand the last feature was the promotion\nwithin last three years and there's also\na big difference so if you look at\npeople which are leaving the company\nthey are not have a promotion in the\nlast three years so it's not uh it's\nreally huge difference and if you look\nat the people which are still staying um\nthey have they have to have around 5 000\nemployees they are promote they have a\npromotion and 11 uh near 12 000\nemployees they have no promotion still\nstaying but that's also like something\nwhat you can do so if you have high\nlevel skilled employees you can prevent\nthem maybe with promotion\nand yeah so that's so that's the\naddition what you can do if you\num yeah if you detect uh employees which\nare which you want or which are maybe\nleaving then you you can um check these\nfeatures and and check\num what what have they done and then you\ncan hopefully prevent the employee\num for training\nso yeah that was my solution and thank\nyou\nyes\n[Music]\nthank you Clements thanks for sharing\nyou can and share and let's all go back\non camera for the Q a\nperfect timing okay\nyeah I'm back\nbrilliant\npayments used to share it uh yes\num\nthere we are it's off yeah everyone's\nback\nthank you everyone uh to you\num for sharing your presentations very\ninteresting\nthat the HR training can actually be\nnegatively impacting employees to to\nleave a company interesting\num any comments from our expert team\nhere on the presentations\nforeign\nfabulous uh presentations again I think\nthat was really really cool by by all of\nyou I think very interesting also to see\nthe the different kind of the different\napproaches you you've been taking to to\ntackle the challenge so\num I think everybody even today learned\na lot uh about you know how easy or\nmaybe challenging it was to to tackle\nthe ml Challenge and actually I would\nhave a question I mean you've all been\nnew to Hana machine learning and the\npython interface and used Open Source\nbefore and I think you commented a\nlittle bit about it already but I would\nreally like to uh learn maybe how easy\ndid you did you find it to maybe go from\nsomething like psychic learn and pandas\ndata frame to to the Hana data frame and\nthe Hana machine learning uh function so\nI it looks kind of uh you did a great\njob and explored it in in such a detail\nI would have never expected so that's\nfantastic but was it actually a big\nchallenge for you or did you find it\neasy to to Pivot let's say from Psychic\nlearn to to Hanuman\nso Christopher I can start\nand uh\nthe editors also that I was trying to\nthink about using tensorflow and all all\nthose things for for this challenge but\nagain then I discuss with you and\nrealize that the intent was to use sap\nML and I was a bit reluctant initially\nbut again when I started using I found\nthat it was way it was very easy there\nwas a small learning curve of course but\nagain I think the uh the what we have in\nsap ml is is really uh really highly\nuseful and uh pretty concise also one\nthing that I thought could be done\nbetter is that there is some I think\nthere is some lacking documentation uh\nthat would have made my life much more\neasier but again it was not not at all\nhard but again some more documentation\nwould help other Learners to uh you know\nramp up more quickly\nthank you\nmaybe maybe a comment from Sergio or\nClements on this as well\nyeah it took me about three days to\nunderstand how it works because I had\nthe previously different uh\nproof of Concepts I even built something\nwith about just as a proof of concept\nand the main", "doc_id": "cd5e6ece-3314-4112-b414-896f3d7678f1", "embedding": null, "doc_hash": "87cac896e943dc9f4541fff05851f609ea704be63be39c810d934ee6ba65321b", "extra_info": null, "node_info": {"start": 35831, "end": 39884, "_node_type": "1"}, "relationships": {"1": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44", "2": "50bdd7e6-2293-4b23-943f-e3ace9abd902", "3": "35faf7a1-2577-4d2d-b4bc-8a41ec790a5a"}}, "__type__": "1"}, "35faf7a1-2577-4d2d-b4bc-8a41ec790a5a": {"__data__": {"text": "Concepts I even built something\nwith about just as a proof of concept\nand the main part was about data frame\nbecause it's a little bit different but\nmachine but about machine learning\nlibraries they are very well aligned\nwith the open source libraries\nso as as you've seen in the thread it\ntook me about three days to to get the\nidea and then to to get even more\nexamples different pieces of code and\nthen to put everything in function and\nrun everything from one end to the end\nokay\nthank you\nyeah from my side it was also\num the same so the data frames was a\nlittle different and you have to get\nknow it but the learning curve is very\num steep so you're learning pretty fast\nthe the handling of the tool and you\nalso have the advantage then that you\ncan run it on the server and you don't\nhave to run it locally so it's also like\na beneficial for you that that you you\nare using the the sub animal data frames\nbut they're also like providing a lot of\nfeatures with SQL um to to get the views\nof the data\num and the Machine running is pretty the\nsame so also they create search and so\non all just different\num yeah functions name but the most are\npretty the same so it was also easy for\nme yes\nthank you\nI'd also like to comment one thing oh so\nSarah to go go ahead yeah yeah there is\none more thing I um I noticed there are\nsome functions that are embedded into\nthe models for instance if you'll use\nopen source uh HD boost you'll have to\nencode features but in hybrid you don't\nhave to import features and also if you\nuse HD open source YouTube boost you\nhave to this you have to specify all the\nfeatures but in hybrid you don't have to\nspecify the features the exact number of\nfeatures it will take it from the\ninitial data so I didn't have time to\nexplore everything but I I've noticed\nthat there are a lot of function\nembedded in in library otherwise in open\nsource you'll have to to handle it by\nyourself\nokay thank you cool\num my comment was about giving kudos to\nthe sap Hana Cloud expert team because I\nthink\nit's interesting to to know how long it\ntook you to read into it um Etc and I\nthink in addition to you being brilliant\nto invest your time in like reading into\nit and doing the challenge I think it\nwas brilliantly organized by the sap\nHana Cloud team to offer open Office\nhours to everyone in the sap community\nthat had questions regarding the\nchallenge or questions in general about\nmachine learning so they they also\ninvested a lot of time\num out of their busy schedules to uh to\nbe there for the sap community and to\noffer their help and knowledge to\neveryone so big Kudos also to to\neveryone here on the organ team\num with that being said I also just um\nas Christoph mentioned in the beginning\num\npasted Susan's summary announcement blog\npost into that chat for everyone to\nreread\num and I'd like to thank you all for\ntaking your time today\num it's been a pleasure to have you here\non our sap Community call I hope you\nenjoyed it too\nbig thumbs up\nokay great\num\nlaughs\nright yes\nyou were on mute Sergio yeah I was a\nmute I I I\nI'm alive I have hands that's all I'm\nnot I'm not a competition intelligent\nI'm just alive being all right okay then\nthanks everyone take good care and I\nhope to see you in our next\num potential sap Hana Cloud machine\nlearning challenge\nbye\n", "doc_id": "35faf7a1-2577-4d2d-b4bc-8a41ec790a5a", "embedding": null, "doc_hash": "371b0ed272ccb808de30c75226682bf78516ec9f7c820538bbc013ec2d743af6", "extra_info": null, "node_info": {"start": 39872, "end": 43163, "_node_type": "1"}, "relationships": {"1": "deada3d5-c518-4a3e-97b7-25e9c7c6ff44", "2": "cd5e6ece-3314-4112-b414-896f3d7678f1"}}, "__type__": "1"}}, "docstore/ref_doc_info": {"deada3d5-c518-4a3e-97b7-25e9c7c6ff44": {"doc_ids": ["ef9169da-6b0a-4c32-9d93-9e1753565348", "53522092-dbec-426e-a779-76ba974d2769", "45800f4a-e312-4f64-9e93-4b7ffaf72aea", "b5c9b7cc-cd21-441c-9ef4-8fdcd9b8c07a", "7d088e76-4cd8-468f-b196-1effbab799f2", "c7a0b19c-68c0-4301-85c1-e17a638446e5", "3735dcec-96a3-4c2d-97c5-c50784a8587c", "4142bedf-19a9-4931-aa14-ba468af9ec83", "50bdd7e6-2293-4b23-943f-e3ace9abd902", "cd5e6ece-3314-4112-b414-896f3d7678f1", "35faf7a1-2577-4d2d-b4bc-8a41ec790a5a"], "extra_info": {}}}}