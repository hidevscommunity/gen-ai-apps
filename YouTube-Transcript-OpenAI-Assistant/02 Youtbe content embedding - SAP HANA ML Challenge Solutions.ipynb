{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f823fa1",
   "metadata": {},
   "source": [
    "### SAP Machine Learning Embedding in OpenAI\n",
    "##### Author: Sergiu Iatco. June, 2023\n",
    "https://people.sap.com/iatco.sergiu <br>\n",
    "https://www.linkedin.com/in/sergiuiatco/ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a62a4",
   "metadata": {},
   "source": [
    "#### Resources:\n",
    "https://pypi.org/project/gpt-index/ <br>\n",
    "https://github.com/jerryjliu/llama_index/blob/main/examples/langchain_demo/LangchainDemo.ipynb <br>\n",
    "https://github.com/jerryjliu/llama_index/tree/main/examples <br>\n",
    "https://github.com/jerryjliu/llama_index/blob/main/examples/vector_indices/SimpleIndexDemo-ChatGPT.ipynb <br>\n",
    "https://gpt-index.readthedocs.io/en/stable/reference/service_context.html <br>\n",
    "https://gpt-index.readthedocs.io/en/stable/reference/service_context/embeddings.html <br>\n",
    "https://gpt-index.readthedocs.io/en/stable/getting_started/starter_example.html store and load <br>\n",
    "https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html <br>\n",
    "\n",
    "https://blog.streamlit.io/how-to-build-an-llm-powered-chatbot-with-streamlit/ <br>\n",
    "https://github.com/dataprofessor/langchain-text-summarization <br>\n",
    "https://github.com/dataprofessor <br>\n",
    "\n",
    "Blogs: <br>\n",
    "https://blogs.sap.com/2022/11/07/sap-community-call-sap-hana-cloud-machine-learning-challenge-i-quit-how-to-prevent-employee-churn/ <br>\n",
    "https://blogs.sap.com/2022/11/28/i-quit-how-to-predict-employee-churn-sap-hana-cloud-machine-learning-challenge/ <br>\n",
    "https://blogs.sap.com/2022/12/22/sap-hana-cloud-machine-learning-challenge-2022-the-winners-are/ <br>\n",
    "\"I quit!\" - How to prevent employee churn | SAP Community Call | Kick-off <br>\n",
    "https://www.youtube.com/watch?v=pgV_NFdokZ4 <br>\n",
    "\"How to prevent Employee Churn using SAP HANA Cloud | SAP Community Call | Solutions\" <br>\n",
    "https://www.youtube.com/watch?v=ul5ZqnB3qVw <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc4bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac381f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.core.debugger import set_trace\n",
    "# os.environ[\"OPENAI_API_KEY\"] = '<OPENAI_API_KEY>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd5a045",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "import shutil\n",
    "import pathlib\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.CRITICAL)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# There are five standard levels for logging in Python, listed here in increasing order of severity:\n",
    "# DEBUG: Detailed information, typically of interest only when diagnosing problems.\n",
    "# INFO: Confirmation that things are working as expected.\n",
    "# WARNING: An indication that something unexpected happened or indicative of some problem in the near future (e.g., ‘disk space low’). The software is still working as expected.\n",
    "# ERROR: Due to a more serious problem, the software has not been able to perform some function.\n",
    "# CRITICAL: A very serious error, indicating that the program itself may be unable to continue running.\n",
    "\n",
    "class llama_context():\n",
    "    def __init__(self, path=None):\n",
    "        \n",
    "        if path!=None:\n",
    "            self.path = path\n",
    "        else:\n",
    "            self.path = ''\n",
    "        \n",
    "        perisit_sub_dir = \"storage\"\n",
    "        self.perisit_dir = os.path.join(self.path, perisit_sub_dir)\n",
    "        if not os.path.exists(self.perisit_dir):\n",
    "            os.makedirs(self.perisit_dir)\n",
    "        data_sub_dir = \"data\"\n",
    "        self.data_dir = os.path.join(self.path, data_sub_dir)\n",
    "        self.data_dir_counter = 0\n",
    "        \n",
    "        self.cost_model_ada = \"ada\" # https://openai.com/pricing\n",
    "        self.cost_model_davinci = \"davinci\" # https://openai.com/pricing\n",
    "        self.price_ada_1k_tokens = 0.0004\n",
    "        self.price_davinci_1k_tokens = 0.03 \n",
    "\n",
    "        \n",
    "    def load_data(self):\n",
    "        self.documents = SimpleDirectoryReader(self.data_dir).load_data()\n",
    "        print(f\"Documents loaded: {len(self.documents)}.\")\n",
    "    def create_vector_store(self):\n",
    "        self.index = GPTVectorStoreIndex.from_documents(self.documents)\n",
    "        print(\"GPTVectorStoreIndex complete.\")\n",
    "    def save_index(self):\n",
    "        self.index.storage_context.persist(persist_dir=self.perisit_dir)\n",
    "        print(f\"Index saved in path {self.perisit_dir}.\")\n",
    "    def load_index(self):\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=self.perisit_dir)\n",
    "        self.index = load_index_from_storage(storage_context)\n",
    "    def start_query_engine(self):\n",
    "        self.query_engine = self.index.as_query_engine()\n",
    "        print(\"Query_engine started.\")\n",
    "    def post_question(self, question, sleep = None):\n",
    "        if sleep == None:\n",
    "            self.sleep = 0 # trial 20s\n",
    "        self.response_cls = self.query_engine.query(question)\n",
    "        self.response = self.response_cls.response\n",
    "\n",
    "    def del_data_dir(self):\n",
    "        path = self.data_dir\n",
    "        try:\n",
    "            shutil.rmtree(path)\n",
    "            print(f\"{path} deleted successfully!\")\n",
    "        except OSError as error:\n",
    "            print(f\"Error deleting {path}: {error}\")\n",
    "\n",
    "    def copy_file_to_data_dir(self, file_extension ='.txt', verbose = 0):\n",
    "\n",
    "        path_from = self.path\n",
    "        path_to = self.data_dir\n",
    "\n",
    "        if not os.path.exists(path_to):\n",
    "            os.makedirs(path_to)\n",
    "\n",
    "        for filename in os.listdir(path_from):\n",
    "            if filename.endswith(file_extension):\n",
    "                source_path = os.path.join(path_from, filename)\n",
    "                dest_path = os.path.join(path_to, filename)\n",
    "                shutil.copy(source_path, dest_path)\n",
    "                if verbose == 1:\n",
    "                    print(f\"File {filename} copied successfully!\")\n",
    "    \n",
    "        path_to_lib = pathlib.Path(path_to)\n",
    "        path_to_lib_files = path_to_lib.glob(f\"*{file_extension}\")\n",
    "        print(f\"Files {len(list(path_to_lib_files))} copied in {path_to}.\")\n",
    " \n",
    "    def copy_path_from_to_data_dir(self, path_from, file_extension ='.txt', verbose = 0):\n",
    "\n",
    "        path_to = self.data_dir # default data folder for llama\n",
    "        start_counter = self.data_dir_counter\n",
    "        \n",
    "        if not os.path.exists(path_to):\n",
    "            os.makedirs(path_to)\n",
    "\n",
    "        padding_n = 5\n",
    "        path_from_lib = pathlib.Path(path_from)\n",
    "        path_from_lib_files = path_from_lib.glob(f\"**/*{file_extension}\")\n",
    "\n",
    "        files_copied_n = 0\n",
    "        counter = None\n",
    "        for counter, file in enumerate(path_from_lib_files, start_counter):\n",
    "            filename_path = os.path.split(file)[0] # path only\n",
    "            filename = os.path.split(file)[1] # filename only\n",
    "            filename_with_index = f'{str(counter).zfill(padding_n)}_{filename}'\n",
    "            file_to_data_dir = os.path.join(path_to, filename_with_index)\n",
    "            shutil.copy(file, file_to_data_dir)\n",
    "            \n",
    "            if os.path.exists(file_to_data_dir):\n",
    "                files_copied_n += 1\n",
    "                if verbose == 1:\n",
    "                    print(f\"File {filename} -> copied successfully!\")\n",
    "            else:\n",
    "                if verbose == 1:\n",
    "                    print(f\"File {filename} was not copied!\")\n",
    "        \n",
    "#         if 'counter' in locals(): \n",
    "        if counter != None: \n",
    "            self.data_dir_counter = counter + 1 # start from last\n",
    "        \n",
    "        print(f\"Files: {files_copied_n} copied to folder: {path_to}!\")\n",
    "\n",
    "    def estimate_tokens(self, text):\n",
    "        words = text.split()\n",
    "\n",
    "        num_words = int(len(words))\n",
    "        tokens = int(( num_words / 0.75 ))\n",
    "        tokens_1k = tokens / 1000\n",
    "        cost_ada = tokens_1k * self.price_ada_1k_tokens\n",
    "        cost_davinci = tokens_1k * self.price_davinci_1k_tokens\n",
    "        return tokens, cost_ada, cost_davinci\n",
    "    \n",
    "    def estimate_cost(self):\n",
    "        total_tokens = 0\n",
    "        total_cost_ada = 0\n",
    "        total_cost_davinci = 0\n",
    "        costs_rounding = 8\n",
    "        \n",
    "        for doc in self.documents:\n",
    "            text = doc.get_text()\n",
    "            tokens, cost_ada, cost_davinci = self.estimate_tokens(text)\n",
    "            total_tokens += tokens\n",
    "            \n",
    "            total_cost_ada += cost_ada\n",
    "            total_cost_ada = round(total_cost_ada, costs_rounding)\n",
    "            \n",
    "            total_cost_davinci += cost_davinci\n",
    "            total_cost_davinci = round(total_cost_davinci, costs_rounding)\n",
    "            \n",
    "        self.total_tokens = total_tokens\n",
    "        self.total_cost_ada = total_cost_ada\n",
    "        self.total_cost_davinci = total_cost_davinci\n",
    "        print(f\"Total tokens: {self.total_tokens}\")\n",
    "        print(f\"Total estimated costs with model {self.cost_model_ada }: ${self.total_cost_ada}\")\n",
    "        print(f\"Total estimated costs with model {self.cost_model_davinci }: ${self.total_cost_davinci}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78c84ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "YoutubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\")\n",
    "loader = YoutubeTranscriptReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f43797eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytb_name = 'ytb_hana_ml_call_20230126'\n",
    "ytb_link = 'https://www.youtube.com/watch?v=ul5ZqnB3qVw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ea276e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "welcome everyone\n",
      "Welcome to our sap Community call today\n",
      "I'm glad to see so many of you here in\n",
      "our call\n",
      "um very special sap Community call it's\n",
      "a winner announcement of our sap Hana\n",
      "Cloud machine learning Challenge and I'm\n",
      "more than excited to be joined here\n",
      "today by our three winners of our\n",
      "challenge\n",
      "um I'm Lena from The SFU Community team\n",
      "and we're hosting this call today with\n",
      "the sap Hana Cloud team\n",
      "so um let me introduce the winners to\n",
      "you\n",
      "um it's vishwas madhubashi Sergio yatko\n",
      "and Clements Vera\n",
      "who made it here on our virtual stage\n",
      "and won the challenge and our today able\n",
      "to present to you each for 15 minutes\n",
      "how they answered the main question\n",
      "namely how to prevent employee churn by\n",
      "using sap Hana Cloud's machine learning\n",
      "capabilities\n",
      "the challenge ran for three weeks in the\n",
      "sap Community back in November until\n",
      "mid-December and we had quite\n",
      "interesting solution paths but you three\n",
      "made it so congratulations here\n",
      "officially to every one of you\n",
      "um before we start and before I head\n",
      "over to\n",
      "um Krista from our sap Hana Cloud expert\n",
      "team to give you a little bit more\n",
      "background on what the challenge really\n",
      "was about\n",
      "um let me briefly emphasize on um the\n",
      "fact that we have time after\n",
      "um vishwas selju and Clements presented\n",
      "to all of you\n",
      "um to address any questions that might\n",
      "come up\n",
      "um during the presentations or and that\n",
      "you have in general with regards to sap\n",
      "Hana Cloud machine learning so we have\n",
      "10 to 15 minutes towards the end of this\n",
      "call to address these and therefore\n",
      "please log into YouTube here\n",
      "um and type the questions in the chat\n",
      "I'm already also let us know where we're\n",
      "tuning in from how are you feeling today\n",
      "and what's on your mind so let's have a\n",
      "lively chat here today in our YouTube\n",
      "call\n",
      "and with that being said I'll hand over\n",
      "to Crystal to share his screen\n",
      "um to give a little bit more background\n",
      "before then uh which of us will start\n",
      "with his first presentation\n",
      "thank you so much Lena for the warm\n",
      "introduction and welcome I just wanted\n",
      "to quickly uh uh recap what the\n",
      "challenge was about uh how to predict\n",
      "employee churn so we provided a data set\n",
      "about employee data with different\n",
      "features and certainly kind of an\n",
      "indicator that people uh kind of left\n",
      "the company Flight Risk was the what's\n",
      "the column name and then the challenge\n",
      "was for the participants of the\n",
      "challenge we had about 20 people kind of\n",
      "actively participating and running their\n",
      "machine learning scenarios using the\n",
      "Hana Cloud machine learning libraries\n",
      "and capabilities to come up with a\n",
      "prediction model which would then help\n",
      "the HR department to yeah in the end\n",
      "prevent employee churn so the idea was\n",
      "kind of what can we find out in the data\n",
      "and guide and help the HR team to\n",
      "prevent\n",
      "employees in the different sections of\n",
      "the company to not leave the company so\n",
      "for everybody who couldn't participate\n",
      "in the actual challenge there are a\n",
      "couple of links here which will also\n",
      "share and which you will certainly also\n",
      "find searching for them the initial\n",
      "block uh we have a sample repository\n",
      "where we provided the data so it's\n",
      "available for you to also kind of pick\n",
      "up the challenge even now or later in\n",
      "our uh Hanam samples repository we had\n",
      "lots of great questions during the\n",
      "community challenge in our community\n",
      "thread I've also shared the link here\n",
      "and certainly then we have the the\n",
      "winners plot which Susan our colleague\n",
      "created and\n",
      "really now with no further Ado we want\n",
      "to kind of learn uh from our winners\n",
      "um how they approach The Challenge and\n",
      "what were their solutions to the\n",
      "challenge\n",
      "and with that I hand over to vishwas for\n",
      "uh his first solution presentation\n",
      "thank you Kristoff so let me start\n",
      "sharing my screen and talk through the\n",
      "solution\n",
      "can you see my screen now\n",
      "yes we can\n",
      "okay\n",
      "so the the challenge was to predict the\n",
      "employee churn and for that we had some\n",
      "data and some uh some very very very\n",
      "clear uh objective let's see what the\n",
      "data was and what the objective was\n",
      "so\n",
      "the\n",
      "context\n",
      "the context was that the companies want\n",
      "to retain their High performing\n",
      "employees and the to reduce the churn\n",
      "rate of those employees and right now\n",
      "the managers uh try to think about who\n",
      "can leave and they they try to prevent\n",
      "prevent the addition of those employees\n",
      "but this is very subjective not based\n",
      "upon data and can be wrong most of the\n",
      "times so the intent was to\n",
      "take a look at the concrete data and\n",
      "analyze the patterns of the employee's\n",
      "churn and figure out which are the\n",
      "employees who are highly likely to at\n",
      "right and stop them from leaving the\n",
      "company if they are valuable to the\n",
      "company\n",
      "managers will continue using their\n",
      "intuition for preventing the employee\n",
      "churn but the intent of the challenge\n",
      "was to figure out which models can be\n",
      "developed to predict the churn of the\n",
      "employees at certain events certain\n",
      "certain times certain areas and make\n",
      "sure that we can take the necessary\n",
      "steps before the chair becomes obvious\n",
      "before they put in their paperwork and\n",
      "use mathematics and data to arrive at\n",
      "the conclusion that we can rely upon\n",
      "for this objective we were given about\n",
      "a big set of data we had about 90 000\n",
      "samples uh with the 42 independent\n",
      "variables and we had one dependent\n",
      "variable called the Flight Risk out of\n",
      "these samples we were able to figure out\n",
      "a few obvious groups and we have marked\n",
      "them in on the screen right now one\n",
      "group was about the age we had age age\n",
      "group 10 age group 5 and generation so\n",
      "we had we were able to reduce some of\n",
      "this data based upon uh finding by\n",
      "finding the most obvious one most useful\n",
      "one then we had a second group for the\n",
      "previous functional area previous job\n",
      "level all those things from the previous\n",
      "previous uh previous job uh then we have\n",
      "one group about the that location we had\n",
      "current country latitude longitude\n",
      "regions we were able to drop some points\n",
      "from here as well and finally we had the\n",
      "scientist as the fourth uh fourth Point\n",
      "here which was the dependent variable\n",
      "the data was synthetically generated but\n",
      "it was supposed to mimika real life\n",
      "scenario\n",
      "uh the Eda insights for this uh for this\n",
      "were our pretty interesting\n",
      "what we found that 8 out of the 42\n",
      "independent variables have equal number\n",
      "of missing values which meant that these\n",
      "uh there was something very common in\n",
      "these variables and you were able to\n",
      "figure that they were they had all the\n",
      "uh all the same requires for had all the\n",
      "same missing values we were able to drop\n",
      "some of the records to make sure that\n",
      "you are not counting uh the records of\n",
      "the missing data and we were not able to\n",
      "generate those because they meant they\n",
      "were related to the previous jobs since\n",
      "we could not uh create the editor we had\n",
      "to drop some records\n",
      "one uh one interesting uh and kind of uh\n",
      "disturbing thing was we were able we we\n",
      "found that there was some potential\n",
      "anomaly in the data\n",
      "uh we saw that the 71.5 rules had been\n",
      "more critical\n",
      "and by 28 were marked as non-critical so\n",
      "this seemed like an annually to us and\n",
      "we have uh pointed this out to sap and\n",
      "uh hopefully uh with the correction of\n",
      "data we can arrive at the better models\n",
      "but this was one of the obvious outliers\n",
      "that we saw in the data itself\n",
      "correlations were not there but not many\n",
      "we could not we don't did not find any\n",
      "significant correlation in the\n",
      "mathematical columns the only two small\n",
      "correlations we found were in the uh\n",
      "time time in previous previous question\n",
      "month and promotion within last three\n",
      "years but the correlations were very\n",
      "minimal uh just about\n",
      "0.15.17 not too high so these\n",
      "correlations were not really significant\n",
      "so they did not take away anything from\n",
      "this part\n",
      "one thing that we found was that the age\n",
      "and job type were kind of correlated\n",
      "with the attrition as the age increased\n",
      "the attrition started dropping\n",
      "in the 40 50 in the 55 plus group the\n",
      "addition rate virtually drops to zero\n",
      "while it is highest in the 20 to 30\n",
      "group there was one outlier though uh\n",
      "the 45 250 group but again in general\n",
      "the increase in age indicated the\n",
      "admission dates rates kept going down\n",
      "with that\n",
      "uh the other was that the temporary\n",
      "employees were more likely likely to\n",
      "attract and there is no surprise uh so\n",
      "this was obvious and this is visible in\n",
      "the uh in the uh in the graph at the\n",
      "bottom as well\n",
      "function and region were also a small\n",
      "scale indicator of the attrition we saw\n",
      "that the people in sales and marketing\n",
      "were at a higher risk of its writing and\n",
      "that's what we have marked in the uh in\n",
      "the uh in the graph above let me just\n",
      "highlight that as well\n",
      "it's this part we also saw that the apj\n",
      "region had the highest attrition rate\n",
      "nearly double than than other regions so\n",
      "I think there may be something going on\n",
      "in the APG region so I think the apj\n",
      "region and these sales and marketing\n",
      "cutting departments had something going\n",
      "on that was contributing to the highest\n",
      "higher education rates so this is one\n",
      "area that we should take a look at\n",
      "we also found that the decreasing\n",
      "performance rating was a good indicator\n",
      "of attrition so no surprise here and\n",
      "also a demotion was a good indicator of\n",
      "the of the higher iteration so no\n",
      "surprise here the the ID is do we even\n",
      "want to stop these employees if somebody\n",
      "has a decreasing performance do you want\n",
      "to stop those employees I think we have\n",
      "to talk about the HR to take a look at\n",
      "the next next action for these employees\n",
      "or pre-processing we filled all the nand\n",
      "values with an assigned and then we\n",
      "split the data and data into 10 desktop\n",
      "validation in the 80 10 10 ratio and\n",
      "then we did all the uh all the\n",
      "preprocessing and uh and we went ahead\n",
      "with the model building\n",
      "we found that the sap made the life of a\n",
      "data scientist very easy\n",
      "had we not had all the functions\n",
      "available within sap we had to go\n",
      "through all these\n",
      "all these steps dropping 12 columns\n",
      "including pen columns mapping zero mean\n",
      "for the binary values getting dummies so\n",
      "all those things were kind of not\n",
      "required with sap so this was I think a\n",
      "lot of times it happened to us because\n",
      "we were using Sap's machine learning\n",
      "machine learning solution\n",
      "let's talk and take a look at the model\n",
      "insights\n",
      "so for the first model we use the\n",
      "gradient boosting binary classifier\n",
      "and with this one we were able to and\n",
      "again since we are trying to create the\n",
      "predict the churn we are looking at\n",
      "recall as the the value of interest uh\n",
      "with this model the recall value that\n",
      "you got was a low value just about 55\n",
      "and one percent not really encouraging\n",
      "accuracy was good decision was not not\n",
      "really uh encouraging either an FN score\n",
      "was low AUC boss it was decent but again\n",
      "not really uh but this model overall was\n",
      "not not really encouraging\n",
      "uh one thing that we point out here is\n",
      "that critical job role was low\n",
      "contributory here and again this is one\n",
      "more thing that I have pointed out\n",
      "earlier that this is one area that may\n",
      "have uh impacted our models and they\n",
      "should take a second look at this one\n",
      "also one part is that salary was about I\n",
      "think ninth or 10th in the hierarchy so\n",
      "funny enough salary has not featured as\n",
      "the top performing in any of the models\n",
      "so salary is not the core concern of\n",
      "employers who are leaving the job uh\n",
      "there are other factors we have to take\n",
      "a look at those more uh more\n",
      "interestingly\n",
      "since model 1 was not really uh\n",
      "impressive we went on uh to build two\n",
      "more models so for the model 2 we used\n",
      "the random forest classifier and in this\n",
      "model we were able to achieve a recall\n",
      "value of nearly 94 percent so this was\n",
      "better than the previous one accuracy\n",
      "was also reasonably good reference code\n",
      "was was decent diesel was Precision was\n",
      "not really high but again given that\n",
      "recall is very high prison was supposed\n",
      "to go uh low up a bit which is to be\n",
      "expected in this also the salary is\n",
      "far below so I think we can safely say\n",
      "that salary is usually not the highest\n",
      "reason why the employee script there are\n",
      "various other things that we should\n",
      "should take a look at facility is not\n",
      "one of those and I don't see the uh\n",
      "be uh a critical role here in this at\n",
      "all so I think critical role has has\n",
      "done something uh the data in critical\n",
      "has done something to these models and\n",
      "we should uh we should make sure to to\n",
      "see uh that we have the right values for\n",
      "the critical role for all the employees\n",
      "so we also build one more model to\n",
      "ensure that we are on the right track so\n",
      "for the model 3 we use the hybrid\n",
      "creation boost gradient boosting three\n",
      "and with this one we are able to achieve\n",
      "at lower recall value of 56.25 not\n",
      "really not really uh encouraging\n",
      "accuracy was uh just about six to six\n",
      "please signals are very low F1 was very\n",
      "low so this is not really a good model\n",
      "and somehow the ROC curve uh stooped\n",
      "here in between this was supposed to be\n",
      "smooth here but this two tier for some\n",
      "reason I we have then we have to do some\n",
      "more more research in this area uh but\n",
      "again this model was not also performing\n",
      "at the level expected so we did not uh\n",
      "think that this was a great model to\n",
      "move ahead with in this also salary was\n",
      "pretty low uh so the syllabus number\n",
      "seven here I guess uh but also the uh\n",
      "I don't even see\n",
      "the critical role is not showing up here\n",
      "it's very low so so salary is very low\n",
      "in this in the rating and critical job\n",
      "rule is also fairly low in this area as\n",
      "well\n",
      "for deployment part uh we believe that\n",
      "once we choose the right model we have\n",
      "to look at several factors that can lead\n",
      "to uh leap to employ electrician and I\n",
      "have listed 10 factors here which came\n",
      "from a vented Circle so these 10 factors\n",
      "I'm pretty much sure that the HR is\n",
      "looking at these factors and making sure\n",
      "that these are taken care of\n",
      "but the intent was to ensure that the\n",
      "best employees stay with us we have to\n",
      "make sure that we run this model on\n",
      "certain critical events\n",
      "and I have listed some events here uh\n",
      "the annual appraisal is one critical\n",
      "event promotion is a is a critical event\n",
      "function slash Department change is also\n",
      "a very critical event\n",
      "uh an employee's birthday as the\n",
      "employees age their attrition Tendencies\n",
      "go down\n",
      "any major life event for an employee\n",
      "marriage separation birthday so these\n",
      "are all the events where the employees\n",
      "are likely to make uh changes to their\n",
      "intent to it right and the idea is that\n",
      "the HR should run this model for these\n",
      "employees at these critical events and\n",
      "ensure what has what has changed and\n",
      "based upon that they should take some\n",
      "action what he promote is that we should\n",
      "generate a we should run these reports\n",
      "and Report weekly and the HR should get\n",
      "this report by a in a pushed model\n",
      "notable model so they should get the\n",
      "report and based on that report they\n",
      "should Define the actions for the next\n",
      "few months few weeks few days few years\n",
      "based upon what has happened and uh\n",
      "based upon that they should figure out\n",
      "what they have to do to ensure that the\n",
      "employees who are highly likelihood\n",
      "right and that they want to remained\n",
      "want to retain uh are given what they\n",
      "what they what they need to stay in the\n",
      "company\n",
      "tomorrow should not be used in must it\n",
      "should be they should be you know uh\n",
      "very critical to practical to the\n",
      "employees so it should be written only\n",
      "for those employees which have which\n",
      "have seen any major event in the last\n",
      "week or so and the HR should get the\n",
      "report for only those employees and take\n",
      "action based upon that report\n",
      "our conclusion is that the model to the\n",
      "random Forest classifier is the one that\n",
      "has given us the best performance about\n",
      "94 recall value and it makes sense\n",
      "[Music]\n",
      "foreign\n",
      "we should try to get more data and make\n",
      "better model based upon that\n",
      "uh we must ensure to have a feedbook\n",
      "feedback loop uh to ensure that whatever\n",
      "you learn is passed back to the model\n",
      "and the model keeps improving with every\n",
      "iteration of the Run of this model\n",
      "uh as you mentioned the sales and\n",
      "marketing departments and the APG region\n",
      "had some tendency uh has shown the for\n",
      "tendency for people to attract more so\n",
      "we should see what is happening in these\n",
      "regions and this and these departments\n",
      "to ensure that we can uh we can plug the\n",
      "Gap and ensure that the best employees\n",
      "stay with us\n",
      "uh salary is not among top five\n",
      "contributors so we we can test easy that\n",
      "salary is using what the reason why\n",
      "employees employees leave and we should\n",
      "focus on the other factors\n",
      "and finally we should discuss with HR\n",
      "whether we should filter out the\n",
      "predictions of Flight Risk for the for\n",
      "the uh employees who are who have a\n",
      "decreasing performance rating and who\n",
      "have been demoted do we want to even\n",
      "retain them we should talk to HR to\n",
      "ensure that we can take the right action\n",
      "and not burden HR with the uh with a\n",
      "full report of all the employees uh even\n",
      "including those employees that we that\n",
      "the company does not retain so these are\n",
      "these are the findings that we think are\n",
      "are useful for the HR and with this I\n",
      "conclude my presentation and hand over\n",
      "to\n",
      "Kristoff\n",
      "thank you vishwas\n",
      "okay I'll unmute\n",
      "I would share my screen\n",
      "okay\n",
      "can you see my screen yes\n",
      "yes yeah\n",
      "it's okay\n",
      "yeah yeah well\n",
      "thank you thank you\n",
      "um well after\n",
      "um The Challenge I published the entire\n",
      "Jupiter Notebook on GitHub and\n",
      "afterwards in a few weeks in January I\n",
      "also decided to publish a Blog where I\n",
      "explained the main parts from\n",
      "from the Jupiter notebook so I'll try to\n",
      "do my best to explain in my next 15\n",
      "minutes\n",
      "okay so we already know that this is\n",
      "about\n",
      "in about uh\n",
      "we have we already know that we have to\n",
      "predict the employee Chan as already we\n",
      "have mentioned\n",
      "and that\n",
      "I the first step is to install the\n",
      "um Anaconda and create my\n",
      "my environment and I also installed my\n",
      "Hana ml what I like about Hana machine\n",
      "learning is that I can access it from my\n",
      "local Jupiter notebook and I can write\n",
      "the code in Python and this in Python\n",
      "everything is an objects and the Hana\n",
      "machine learning objects are just like\n",
      "any other objects and I can wrap them in\n",
      "function I can use them in lists and\n",
      "dictionary and get a very clean and\n",
      "reusable code\n",
      "so understanding the challenge\n",
      "looking into the data I can see that our\n",
      "data is highly unbalanced imbalanced we\n",
      "have two classes the first class is no\n",
      "class this is a majority class and the\n",
      "next class is the yes class the class we\n",
      "have to predict\n",
      "and this class is our minority class\n",
      "well the next step is to look into data\n",
      "and look into feature distribution\n",
      "I also feel the the missing values did\n",
      "some feature engineering by creating new\n",
      "features\n",
      "so I'm ready to go to my next step and\n",
      "see what I can get from this data so I\n",
      "can build a model to do so I have to\n",
      "split data into train data and test data\n",
      "I used only one function only one\n",
      "Library which I wrote in a function I\n",
      "used only hybrid gradient boosting three\n",
      "so this is a function where I wrapped\n",
      "everything\n",
      "after\n",
      "I fit the model I can look into my\n",
      "statistics well looking just into model\n",
      "statistics\n",
      "uh\n",
      "I can consider myself lucky if I'll take\n",
      "into consideration on the accuracy\n",
      "because I have an accuracy of\n",
      "0.92 but as\n",
      "but as I but as I mentioned above I know\n",
      "that my classes are highly imbalanced so\n",
      "I cannot\n",
      "rely on accuracy I have to rely only on\n",
      "recall and for that\n",
      "I explained I created a new chapter\n",
      "understanding statistics and I explained\n",
      "why in imbalanced classes we have to\n",
      "rely only on recall and what should we\n",
      "do next to reply also on\n",
      "accuracy so this is a very detailed\n",
      "example one\n",
      "recall is about to know and three yes so\n",
      "um\n",
      "I don't know I will not go into details\n",
      "here because it will take some time but\n",
      "we can look into the blog afterwards if\n",
      "you have time\n",
      "so after also I\n",
      "did my prediction I output the\n",
      "prediction statistics and looking into\n",
      "prediction statistics I also can see\n",
      "it's wrong\n",
      "something wrong\n",
      "okay looking into prediction statistics\n",
      "I can see that my recall is zero three\n",
      "point zero three four\n",
      "and that's very poor\n",
      "I will store all these initial results\n",
      "in\n",
      "a local data frame so I can look into\n",
      "all my results at the end\n",
      "so meanwhile we received additional data\n",
      "and with this additional data\n",
      "with this additional data\n",
      "uh\n",
      "I built again the model I joined all\n",
      "this data and looking into the call I\n",
      "can see a very good Improvement the\n",
      "recall is jumping into zero six into 0.6\n",
      "and also looking into prediction\n",
      "statistics I can see that my recall is\n",
      "zero point\n",
      "five seven it's slightly under the model\n",
      "statistic but this is good\n",
      "the next step is to reducing the number\n",
      "of features to do that I have grouped\n",
      "the features by I have grouped features\n",
      "into\n",
      "I have grouped dependent features\n",
      "you can see the the groups and from each\n",
      "group I selected only one feature by\n",
      "importance\n",
      "also I did some\n",
      "adjustments for instance for previous\n",
      "countries for uh for I selected in case\n",
      "of context I prefer to adjust the\n",
      "selection with the previous countries\n",
      "and current countries\n",
      "something is going in my mouse yeah and\n",
      "looking at prediction with the\n",
      "looking at the prediction statistics I\n",
      "can see that my recall for yes is 0.62\n",
      "well\n",
      "probably think we received even more\n",
      "data now we didn't but we have some uh\n",
      "very powerful tool it's mode it's mode\n",
      "is about synthetic minority over\n",
      "sampling technique\n",
      "I will do one more time the I will check\n",
      "on my time the class proportions\n",
      "I can see it here I can see that my plus\n",
      "yes is highly imbalanced and I have to\n",
      "calculate the the maximum amount for um\n",
      "from Ice mode so the max amount is 710\n",
      "well I will apply this function and I\n",
      "will get my training data\n",
      "below and the attaining data are\n",
      "balanced you can see it from the output\n",
      "so I would like to look into my\n",
      "prediction statistics and here we have\n",
      "the big surprise Mighty call yes is 0.8\n",
      "and this is a very big Improvement of my\n",
      "smoke so I'll keep this mode I also can\n",
      "look into my prediction statistics and I\n",
      "can see that my recall is\n",
      "0.79 it's almost the same so I can go to\n",
      "the next step and this is I will call it\n",
      "the kind of a magic stand because I will\n",
      "do double magic I will balance my test\n",
      "data and also I will increase my\n",
      "training data\n",
      "why I should do this balancing because I\n",
      "would like to use also accuracy for my\n",
      "prediction for my uh prediction\n",
      "statistics so you can see that I\n",
      "balanced my test data and at the same\n",
      "time I have increased my uh\n",
      "training data we see an increase here\n",
      "I will apply this mode to the detained\n",
      "data I will use all these classes all\n",
      "this function I built before I can see\n",
      "that my training data uh\n",
      "balanced they are equal\n",
      "well I can look into my prediction\n",
      "statistics and looking into my\n",
      "prediction statistics\n",
      "the recall for yes is jumping is zero\n",
      "point\n",
      "eight two and looking at the support we\n",
      "can see that our support is 210 for now\n",
      "and 210 for yes and that's mean that we\n",
      "can rely on accuracy and my accuracy in\n",
      "this case is 0 8\n",
      "seven zero point eight seven\n",
      "well what I did above is tops mode it\n",
      "was just the first try but I would like\n",
      "also to to explore it more what does it\n",
      "mean I would like to\n",
      "do it step by step by increasing or in\n",
      "each step with 100.\n",
      "and then I will choose from all these uh\n",
      "models I will choose the best one based\n",
      "on my accuracy on my prediction accuracy\n",
      "because as I said above I can rely this\n",
      "moment\n",
      "also on my\n",
      "accuracy now\n",
      "so this is a very intensive process\n",
      "everything is done in Hana\n",
      "well no I'm just sorry for that\n",
      "it was here\n",
      "it was about\n",
      "burning top smooth yeah it took about it\n",
      "took about eight minutes it took about\n",
      "eight minutes and at the end I can look\n",
      "into all the models I've got the models\n",
      "here and accuracy and I can see that\n",
      "this motor is the 700s is the best one\n",
      "now I can move to the next step building\n",
      "top models\n",
      "and the Imperial top models I would like\n",
      "to add the features one by one in order\n",
      "of importance so I also created one\n",
      "function to wrap everything and this is\n",
      "also a very very intensive process it\n",
      "took about 21 minutes well you can relax\n",
      "enjoy a cup of coffee or of tea I did so\n",
      "but the code is already executed here so\n",
      "we are looking just into the data\n",
      "and we have a great surprise yeah\n",
      "because uh we we can see here that the\n",
      "best mode with the best top\n",
      "15 features gives me an accuracy\n",
      "of 0.89\n",
      "I stored all these models and other\n",
      "different dependencies in a dictionary\n",
      "and as I said before I like honey\n",
      "machine learning libraries that\n",
      "everything is object and you can see in\n",
      "this output that the model is just\n",
      "stored in this list along with all\n",
      "others dependencies which are local\n",
      "and\n",
      "I'll go to I have this last\n",
      "um I have this table I have this data\n",
      "frame\n",
      "data frame it was with all the results\n",
      "but I'll switch to GitHub\n",
      "and because I can show here all the\n",
      "progress so what did we have at the\n",
      "beginning yes we have this initial data\n",
      "and our equal yes was only\n",
      "0.34 and then we have additional data\n",
      "and our recall increased and then I try\n",
      "to reduce the number of features and\n",
      "we've got an increase and then I decided\n",
      "to do some manual adjustments and I've\n",
      "got almost the same recall and in the\n",
      "next step I applied this powerful tools\n",
      "mode and this mode you've got this big\n",
      "increase of recall to zero eight\n",
      "well I did the same with important\n",
      "statistics then I decided in the next\n",
      "step to balance my test data and you can\n",
      "see that my support is 210 here and 210\n",
      "here yeah\n",
      "and this at this stage at this step I\n",
      "can\n",
      "rely also on accuracy and my accuracy is\n",
      "zero eight\n",
      "0.87\n",
      "and I will do the same with the best\n",
      "mode\n",
      "and finally I'll do the same with my top\n",
      "teachers and I've got this best result\n",
      "of 0.89\n",
      "switching back to\n",
      "the blog\n",
      "well analyzing top models well the\n",
      "machine learning is not a black box yeah\n",
      "we can ask some questions and the the\n",
      "response the answers to our questions\n",
      "it's the reason quote\n",
      "each prediction I can get some\n",
      "explanation for instance the reason\n",
      "called the top reason quotes for first\n",
      "prediction I think day six day for the\n",
      "next one and training for the next one\n",
      "and so on\n",
      "I can\n",
      "ask even more questions yes and for the\n",
      "same prediction I would like to receive\n",
      "even more answers to give me top five\n",
      "recent quotes\n",
      "and\n",
      "they can do both\n",
      "something is\n",
      "talking I got it okay\n",
      "so we've seen only data above but also\n",
      "we can we can visualize all these\n",
      "statistics we have plots we have a very\n",
      "powerful Library unified report we can\n",
      "see all these importance we have seen\n",
      "before uh as a chart we can see\n",
      "also we can output other\n",
      "statistics model model statistics then\n",
      "we can have explanation the form from\n",
      "the prediction and we can see the\n",
      "contribution of which features and for\n",
      "that we can use shape League planer\n",
      "this is a plot from shapely explainer\n",
      "also everything I\n",
      "described above about uh\n",
      "recall and\n",
      "accuracy we can understand better with\n",
      "the confusion Matrix this is the mode of\n",
      "uh this is a confusion Matrix for model\n",
      "we can take a look at this also\n",
      "normalized\n",
      "and also we can look at the prediction\n",
      "statistics of we can look at confusion\n",
      "Matrix we can look at confusion Matrix\n",
      "of prediction and we can see that we\n",
      "have here pulse no\n",
      "32 and through yes is\n",
      "178 and if we assume that we'll get 201\n",
      "yeah so and above you will get the same\n",
      "uh same value meaning that our test data\n",
      "have been balanced and we can rely on\n",
      "all statistics recall accuracy precision\n",
      "F1 score and so on what about for the\n",
      "steps well Machine learning\n",
      "uh has many steps well we have\n",
      "connection data and then we have to\n",
      "build all these models and the model is\n",
      "the brand but this brand needs the body\n",
      "so the body in our case is an\n",
      "application and we should positive think\n",
      "about how we can deploy these models\n",
      "as I said at the beginning my scope was\n",
      "to\n",
      "provide the different\n",
      "alternatives to the same model so\n",
      "because in in deployment during the\n",
      "deployment I would like to\n",
      "use different models so I'll be able to\n",
      "compare to do some kind of a b tests\n",
      "so that's actually everything about uh\n",
      "my model\n",
      "and my results\n",
      "thank you Sergio you can hand over\n",
      "thanks you can unshare your screen okay\n",
      "and let's continue\n",
      "can you see my screen\n",
      "yes we can okay perfect yeah so hello\n",
      "everyone\n",
      "my name is Clement I'm from Austria I'm\n",
      "a consultant at\n",
      "STD and yeah so I'm Consulting business\n",
      "and analytics Banning\n",
      "and the STC is one of the leading sub\n",
      "service providers in Austria and we are\n",
      "happy\n",
      "to share our expertise inventiveness and\n",
      "passions to get the best out of their\n",
      "business every day from our customers\n",
      "and we also a long-standing partner of\n",
      "sap and covering the entire sub solution\n",
      "portfolio\n",
      "so yeah and I'm also very happy that I\n",
      "got the opportunity to\n",
      "um get in this Challenge and yeah so I\n",
      "want to share my insights about the\n",
      "challenge\n",
      "um the first thing was um that we had\n",
      "today that it is the aim that we want to\n",
      "predict the employees which are\n",
      "um which want to quit but another aim I\n",
      "also wanted to implement is how to do\n",
      "something against like what can the data\n",
      "do that we can do against it\n",
      "and the first thing as a data scientist\n",
      "you're doing and looking into the data\n",
      "and to get a little bit of feeling of it\n",
      "and yeah so I created some numeric\n",
      "um histograms from the numerical\n",
      "variables and also correlation Matrix so\n",
      "you can see here but as\n",
      "um which has mentioned before you can't\n",
      "find something\n",
      "um significant so it's very yeah very\n",
      "clear\n",
      "um and the tender amounts with the age\n",
      "that's very clear that it's positive\n",
      "corrected in the salary with the age so\n",
      "yes and if you look at the numerator\n",
      "there is the only thing I can find was\n",
      "this salary because if you look there is\n",
      "just a huge amount of people who are\n",
      "earning that most of the 120 000. but\n",
      "the other things are pretty much normal\n",
      "distributed and you can't see anything\n",
      "special from it\n",
      "I also tried it to split it up\n",
      "um that um to compare the people who've\n",
      "um quitted the job and who created not\n",
      "so like from the salary or from the age\n",
      "and there was nothing\n",
      "um special significantly so it was\n",
      "almost the same and yes\n",
      "and another thing was the\n",
      "so I have to go to next slide\n",
      "so yes another thing what um was that\n",
      "the data is very imbalanced so you can\n",
      "see that for the company it's very good\n",
      "that the most people are staying and\n",
      "just a small amount of employees are\n",
      "quitting but as a view as a data\n",
      "scientist it's not a really perfect\n",
      "scenario because you really want to find\n",
      "the people who are quitting and if this\n",
      "um is a minority test and it's it's\n",
      "getting harder and the sap pile has an\n",
      "implementation to\n",
      "um\n",
      "balance the data so it's called the\n",
      "small Tech well sorry and\n",
      "um the small Tech is this mode that you\n",
      "um\n",
      "uh doing the the data points which are\n",
      "existing from the yes that you are not\n",
      "duplicating it but you uh\n",
      "um creating synthetic data points and\n",
      "atomic aggregious to to reduce the node\n",
      "class that's what I've done and very\n",
      "aggressively so I almost balance it from\n",
      "yeah one to one\n",
      "and that was uh the first approach\n",
      "before I got through the modeling\n",
      "and then we first started with the\n",
      "modeling I decided for the average\n",
      "creating boosting tree\n",
      "model and it's very nice that if you use\n",
      "the sap power you can also do some\n",
      "research and because it's all calculated\n",
      "in in the server at Hana mil so you can\n",
      "use the compute resources from it\n",
      "and find the best and the optimal and\n",
      "parameters I did a tweet search\n",
      "and I let it run for some time and\n",
      "it's very fast so I don't know anymore\n",
      "it's about 20 minutes 40 minutes and I\n",
      "played a lot around with it and the\n",
      "perfect parameters was for the model one\n",
      "um 0.1 for the learning rate and 120\n",
      "estimators under a split threshold from\n",
      "0.1\n",
      "and\n",
      "the thing is also\n",
      "that\n",
      "um yes but it's that is very nice that\n",
      "you just like can write on the server\n",
      "and calculate it\n",
      "um the first uh and here are all\n",
      "features involved in this so I haven't\n",
      "done something in future selection and\n",
      "here all in it and I got a really nice\n",
      "position from 97 percentage of no and\n",
      "from the yes uh position from 78\n",
      "um 70. free almost\n",
      "so yes this is the detection\n",
      "um that you can find it and here you can\n",
      "also\n",
      "um see the correlation Matrix so from\n",
      "the yes we we got 325 right in just 121\n",
      "um row\n",
      "um what you also can do\n",
      "um and see for the\n",
      "foreign\n",
      "you get the confidence score and you\n",
      "also get the reason code from the model\n",
      "and here you can see why so it's not the\n",
      "black box you can see why um the model\n",
      "is um\n",
      "giving it the score yes for example\n",
      "[Music]\n",
      "which are skipping uh quitting sorry\n",
      "and yes so this is the model one and the\n",
      "idea of the model 2 was to just select\n",
      "the top 15 features which are involved\n",
      "in the model one so for example each art\n",
      "reading functional error change type six\n",
      "days and so on\n",
      "and I have done this again with the with\n",
      "the hardware ingredient boosting tree I\n",
      "did a grid search the optimal parameters\n",
      "was a little bit harder so different so\n",
      "it was like running rate 0.4 estimators\n",
      "was the same with 120 in the split\n",
      "threshold with 0.3\n",
      "but the thing is\n",
      "um I didn't get this good result at the\n",
      "model before so the Precision from now\n",
      "is quite the same with 97 percentage but\n",
      "the Precision with yes\n",
      "um it is a little bit\n",
      "um weaker so uh I just got 68 percentage\n",
      "of it so the model one and the model 2 I\n",
      "would suggest to to use the model one\n",
      "with all the features even if we have\n",
      "some other noise in the data because of\n",
      "other features and so on\n",
      "um but uh the the the class we want to\n",
      "predict is yes and here is like we want\n",
      "to find the most\n",
      "um highest Precision without we want to\n",
      "find all the employees\n",
      "um we're creating so this is my first\n",
      "approach and\n",
      "um the second one is what what can we do\n",
      "now if we can find now the employees\n",
      "which are greeting for greeting and then\n",
      "what can we do and what can we do just\n",
      "if we have the data from it\n",
      "and if we go now here what I have done\n",
      "is I use the top five features come from\n",
      "the model\n",
      "and just looked into the detail what\n",
      "it's different why is the model\n",
      "predicting like that kind of and the\n",
      "first the most top feature is each art\n",
      "reading and it has an influence of 28\n",
      "percentage of the model so it's quite a\n",
      "huge influence in this and the funnest\n",
      "thing or I don't know for the company\n",
      "it's not funny but if you look here on\n",
      "the left side you have the um employees\n",
      "which are quitting and on the right side\n",
      "you have the employees which I'm not\n",
      "creating and if you look at the empress\n",
      "um who are quitting you have a really\n",
      "big amount who did HR training and the\n",
      "small amount who didn't have done the\n",
      "chart reading and if you look at the\n",
      "people at the employees which are still\n",
      "at the company it's it's in the other\n",
      "direction so yeah I don't know what they\n",
      "do in the company the HR training what\n",
      "they are teaching what they are learning\n",
      "but yeah that's this one indicator that\n",
      "it's not really beneficial for the for\n",
      "the company\n",
      "the second\n",
      "um top feature was the functional area\n",
      "change type that has an influence of\n",
      "nine percentage of the model\n",
      "and I did the same so I splitted it into\n",
      "two groups with the flightless gask and\n",
      "Fighters no and here you can see that\n",
      "um one the no change is yeah\n",
      "is also a huge amount in the in the\n",
      "employees which are still there\n",
      "um but if you look at the\n",
      "cross-functional move so there's a\n",
      "significant uh significant difference\n",
      "between the Flight Risk yes and the\n",
      "Flight Risk new group so if you\n",
      "[Music]\n",
      "um\n",
      "so my suggestion is to to\n",
      "um use this in the company and to deploy\n",
      "that cross-functional move to to prevent\n",
      "every\n",
      "then the third feature was the sick days\n",
      "and here um is the same on the left side\n",
      "you see that the people who quit at the\n",
      "shop and on the right side were still at\n",
      "the company and\n",
      "um I also\n",
      "um printed the mean from the flight with\n",
      "no\n",
      "um amount at flight qcs so the green\n",
      "line is the is the mean\n",
      "um from the from the sick days of the\n",
      "people who are still at the company\n",
      "and the the thing is that people who are\n",
      "employees which are quitting the job\n",
      "have more sick days than the the\n",
      "increase we're staying and for me it is\n",
      "like I would suggest as an indicator\n",
      "around 116 days so if the employee has\n",
      "more than 11 Civic tests and maybe yeah\n",
      "it can be that um\n",
      "the employee will quit for example maybe\n",
      "also if I did additional he did each\n",
      "artery\n",
      "um and the fourth was the employment\n",
      "type\n",
      "so\n",
      "um the\n",
      "um the data that was temporary and\n",
      "regular so temporary means that they\n",
      "have a temporary contract and the other\n",
      "one is just a normal country so the\n",
      "temporary is limited to a date and\n",
      "people employees they are greeting their\n",
      "jobs more when they have a temporary\n",
      "Employment contract and looking for it\n",
      "so it's maybe it's logical because they\n",
      "are looking\n",
      "um for another job before the contract\n",
      "expires and then they are switching\n",
      "companies or that can be but it's also\n",
      "they are not happy maybe with it because\n",
      "we can see it in the data that there is\n",
      "a different\n",
      "and the last feature was the promotion\n",
      "within last three years and there's also\n",
      "a big difference so if you look at\n",
      "people which are leaving the company\n",
      "they are not have a promotion in the\n",
      "last three years so it's not uh it's\n",
      "really huge difference and if you look\n",
      "at the people which are still staying um\n",
      "they have they have to have around 5 000\n",
      "employees they are promote they have a\n",
      "promotion and 11 uh near 12 000\n",
      "employees they have no promotion still\n",
      "staying but that's also like something\n",
      "what you can do so if you have high\n",
      "level skilled employees you can prevent\n",
      "them maybe with promotion\n",
      "and yeah so that's so that's the\n",
      "addition what you can do if you\n",
      "um yeah if you detect uh employees which\n",
      "are which you want or which are maybe\n",
      "leaving then you you can um check these\n",
      "features and and check\n",
      "um what what have they done and then you\n",
      "can hopefully prevent the employee\n",
      "um for training\n",
      "so yeah that was my solution and thank\n",
      "you\n",
      "yes\n",
      "[Music]\n",
      "thank you Clements thanks for sharing\n",
      "you can and share and let's all go back\n",
      "on camera for the Q a\n",
      "perfect timing okay\n",
      "yeah I'm back\n",
      "brilliant\n",
      "payments used to share it uh yes\n",
      "um\n",
      "there we are it's off yeah everyone's\n",
      "back\n",
      "thank you everyone uh to you\n",
      "um for sharing your presentations very\n",
      "interesting\n",
      "that the HR training can actually be\n",
      "negatively impacting employees to to\n",
      "leave a company interesting\n",
      "um any comments from our expert team\n",
      "here on the presentations\n",
      "foreign\n",
      "fabulous uh presentations again I think\n",
      "that was really really cool by by all of\n",
      "you I think very interesting also to see\n",
      "the the different kind of the different\n",
      "approaches you you've been taking to to\n",
      "tackle the challenge so\n",
      "um I think everybody even today learned\n",
      "a lot uh about you know how easy or\n",
      "maybe challenging it was to to tackle\n",
      "the ml Challenge and actually I would\n",
      "have a question I mean you've all been\n",
      "new to Hana machine learning and the\n",
      "python interface and used Open Source\n",
      "before and I think you commented a\n",
      "little bit about it already but I would\n",
      "really like to uh learn maybe how easy\n",
      "did you did you find it to maybe go from\n",
      "something like psychic learn and pandas\n",
      "data frame to to the Hana data frame and\n",
      "the Hana machine learning uh function so\n",
      "I it looks kind of uh you did a great\n",
      "job and explored it in in such a detail\n",
      "I would have never expected so that's\n",
      "fantastic but was it actually a big\n",
      "challenge for you or did you find it\n",
      "easy to to Pivot let's say from Psychic\n",
      "learn to to Hanuman\n",
      "so Christopher I can start\n",
      "and uh\n",
      "the editors also that I was trying to\n",
      "think about using tensorflow and all all\n",
      "those things for for this challenge but\n",
      "again then I discuss with you and\n",
      "realize that the intent was to use sap\n",
      "ML and I was a bit reluctant initially\n",
      "but again when I started using I found\n",
      "that it was way it was very easy there\n",
      "was a small learning curve of course but\n",
      "again I think the uh the what we have in\n",
      "sap ml is is really uh really highly\n",
      "useful and uh pretty concise also one\n",
      "thing that I thought could be done\n",
      "better is that there is some I think\n",
      "there is some lacking documentation uh\n",
      "that would have made my life much more\n",
      "easier but again it was not not at all\n",
      "hard but again some more documentation\n",
      "would help other Learners to uh you know\n",
      "ramp up more quickly\n",
      "thank you\n",
      "maybe maybe a comment from Sergio or\n",
      "Clements on this as well\n",
      "yeah it took me about three days to\n",
      "understand how it works because I had\n",
      "the previously different uh\n",
      "proof of Concepts I even built something\n",
      "with about just as a proof of concept\n",
      "and the main part was about data frame\n",
      "because it's a little bit different but\n",
      "machine but about machine learning\n",
      "libraries they are very well aligned\n",
      "with the open source libraries\n",
      "so as as you've seen in the thread it\n",
      "took me about three days to to get the\n",
      "idea and then to to get even more\n",
      "examples different pieces of code and\n",
      "then to put everything in function and\n",
      "run everything from one end to the end\n",
      "okay\n",
      "thank you\n",
      "yeah from my side it was also\n",
      "um the same so the data frames was a\n",
      "little different and you have to get\n",
      "know it but the learning curve is very\n",
      "um steep so you're learning pretty fast\n",
      "the the handling of the tool and you\n",
      "also have the advantage then that you\n",
      "can run it on the server and you don't\n",
      "have to run it locally so it's also like\n",
      "a beneficial for you that that you you\n",
      "are using the the sub animal data frames\n",
      "but they're also like providing a lot of\n",
      "features with SQL um to to get the views\n",
      "of the data\n",
      "um and the Machine running is pretty the\n",
      "same so also they create search and so\n",
      "on all just different\n",
      "um yeah functions name but the most are\n",
      "pretty the same so it was also easy for\n",
      "me yes\n",
      "thank you\n",
      "I'd also like to comment one thing oh so\n",
      "Sarah to go go ahead yeah yeah there is\n",
      "one more thing I um I noticed there are\n",
      "some functions that are embedded into\n",
      "the models for instance if you'll use\n",
      "open source uh HD boost you'll have to\n",
      "encode features but in hybrid you don't\n",
      "have to import features and also if you\n",
      "use HD open source YouTube boost you\n",
      "have to this you have to specify all the\n",
      "features but in hybrid you don't have to\n",
      "specify the features the exact number of\n",
      "features it will take it from the\n",
      "initial data so I didn't have time to\n",
      "explore everything but I I've noticed\n",
      "that there are a lot of function\n",
      "embedded in in library otherwise in open\n",
      "source you'll have to to handle it by\n",
      "yourself\n",
      "okay thank you cool\n",
      "um my comment was about giving kudos to\n",
      "the sap Hana Cloud expert team because I\n",
      "think\n",
      "it's interesting to to know how long it\n",
      "took you to read into it um Etc and I\n",
      "think in addition to you being brilliant\n",
      "to invest your time in like reading into\n",
      "it and doing the challenge I think it\n",
      "was brilliantly organized by the sap\n",
      "Hana Cloud team to offer open Office\n",
      "hours to everyone in the sap community\n",
      "that had questions regarding the\n",
      "challenge or questions in general about\n",
      "machine learning so they they also\n",
      "invested a lot of time\n",
      "um out of their busy schedules to uh to\n",
      "be there for the sap community and to\n",
      "offer their help and knowledge to\n",
      "everyone so big Kudos also to to\n",
      "everyone here on the organ team\n",
      "um with that being said I also just um\n",
      "as Christoph mentioned in the beginning\n",
      "um\n",
      "pasted Susan's summary announcement blog\n",
      "post into that chat for everyone to\n",
      "reread\n",
      "um and I'd like to thank you all for\n",
      "taking your time today\n",
      "um it's been a pleasure to have you here\n",
      "on our sap Community call I hope you\n",
      "enjoyed it too\n",
      "big thumbs up\n",
      "okay great\n",
      "um\n",
      "laughs\n",
      "right yes\n",
      "you were on mute Sergio yeah I was a\n",
      "mute I I I\n",
      "I'm alive I have hands that's all I'm\n",
      "not I'm not a competition intelligent\n",
      "I'm just alive being all right okay then\n",
      "thanks everyone take good care and I\n",
      "hope to see you in our next\n",
      "um potential sap Hana Cloud machine\n",
      "learning challenge\n",
      "bye\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ytb_doc = loader.load_data(ytlinks=[ytb_link])\n",
    "ytb_content = ytb_doc[0].text\n",
    "print(ytb_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2375e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def time_now():\n",
    "    now = datetime.datetime.now()\n",
    "    formatted = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "#     print(formatted)\n",
    "\n",
    "time_now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7578ebdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama_ytb_hana_ml_call_20230126'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'llama_ytb_hana_ml_call_20230126\\\\data'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'llama_ytb_hana_ml_call_20230126\\\\storage'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'llama_ytb_hana_ml_call_20230126\\\\source'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "path_llama = 'llama' + '_' + ytb_name\n",
    "path_from = path_llama + \"\\\\source\"\n",
    "lct = llama_context(path=path_llama)\n",
    "\n",
    "display(lct.path)\n",
    "display(lct.data_dir)\n",
    "display(lct.perisit_dir)\n",
    "display(path_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63cde310",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_from):\n",
    "    os.makedirs(path_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30a3fb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama_ytb_hana_ml_call_20230126\\\\source\\\\ytb_hana_ml_call_20230126.txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = ytb_name + '.txt'\n",
    "ytb_file = os.path.join(path_from, filename)\n",
    "ytb_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "655d7f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ytb_file, \"w\") as file:\n",
    "    file.write(ytb_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "148d2c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "llama_ytb_hana_ml_call_20230126\\data deleted successfully!\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Delete data directory\n",
    "time_now()\n",
    "run_create_save = True\n",
    "if run_create_save:\n",
    "    lct.del_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "717c21a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Files: 1 copied to folder: llama_ytb_hana_ml_call_20230126\\data!\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "time_now()\n",
    "# Copy files from source to data directory\n",
    "run_create_save = True\n",
    "if run_create_save:\n",
    "    lct.copy_path_from_to_data_dir(path_from) # default extension *.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e4345e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['path', 'perisit_dir', 'data_dir', 'data_dir_counter', 'cost_model_ada', 'cost_model_davinci', 'price_ada_1k_tokens', 'price_davinci_1k_tokens'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(lct).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd822712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Documents loaded: 1.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "time_now()\n",
    "# Load documents\n",
    "run_create_save = True\n",
    "if run_create_save:\n",
    "    lct.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bee30ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Total tokens: 11042\n",
      "Total estimated costs with model ada: $0.0044168\n",
      "Total estimated costs with model davinci: $0.33126\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "time_now()\n",
    "# Estimate costs\n",
    "run_create_save = True\n",
    "if run_create_save:\n",
    "    lct.estimate_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f6ad1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "GPTVectorStoreIndex complete.\n"
     ]
    }
   ],
   "source": [
    "# https://platform.openai.com/account/api-keys\n",
    "%time\n",
    "time_now()\n",
    "# Vector create does embedding and costs tokens\n",
    "run_create_save = True\n",
    "if run_create_save:\n",
    "    lct.create_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb6d9c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Index saved in path llama_ytb_hana_ml_call_20230126\\storage.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "time_now()\n",
    "# Save index\n",
    "run_create_save = True\n",
    "if run_create_save:\n",
    "    lct.save_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5472eff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "time_now()\n",
    "# Method load_index() costs as method create_vector_store() but you don't need to upload data\n",
    "run_load = True\n",
    "if run_load:\n",
    "    lct.load_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "952ee2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(lct.index.vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f078570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(lct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cb5b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(lct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ece1cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lct.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a1cb851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Query_engine started.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "time_now()\n",
    "# Start query engine\n",
    "lct.start_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f5b399a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lct.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee02e6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1780 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The content is about a machine learning challenge organized by SAP Hana Cloud to predict employee churn. The challenge involved analyzing a large dataset of 90,000 samples with 42 independent variables and one dependent variable (Flight Risk). The data was synthetically generated to mimic a real-life scenario. The challenge also involved exploring correlations between variables, identifying anomalies in the data, and using open source libraries to build a proof of concept. Finally, the content also acknowledges the SAP Hana Cloud expert team for their help and support in organizing the challenge.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "time_now()\n",
    "question = \"What is content about?\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dc2d62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The three winners of the SAP Hana Cloud Machine Learning Challenge presented their solutions to the challenge of predicting employee churn. Vishwas Madhubashi used a combination of manual adjustments, powerful tools, and important statistics to increase the recall of his model from 0.34 to 0.89. Sergio Yatko used a combination of data balancing, machine learning, and visualization to increase the recall of his model from 0.34 to 0.87. Clements Vera used a combination of machine learning, deployment, and A/B testing to increase the recall of his model from 0.34 to 0.89.\n"
     ]
    }
   ],
   "source": [
    "question = \"Explain presented solutions.\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f676d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Machine Learning\n",
      "- Model\n",
      "- Grid Search\n",
      "- Estimators\n",
      "- Split Threshold\n",
      "- HR Training\n",
      "- Functional Area Change Type\n",
      "- Flight Risk\n",
      "- Sick Days\n",
      "- Employment Type\n",
      "- Unified Report\n",
      "- Shapely Planer\n",
      "- Confusion Matrix\n",
      "- Recall\n",
      "- Accuracy\n",
      "- Precision\n",
      "- F1 Score\n",
      "- Deployment\n",
      "- A/B Tests\n"
     ]
    }
   ],
   "source": [
    "question = \"Extract all technical terms.\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "454b733f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hana Cloud, Hana Cloud Machine Learning, Machine Learning, SAP Hana Cloud, SAP Hana Cloud Machine Learning, Small Tech, Tweet Search, Compute Resources, Feature Selection, Correlation Matrix, Confidence Score, Reason Code.\n"
     ]
    }
   ],
   "source": [
    "question = \"Extract all unique HANA terms. Do not repeat terms.\"\n",
    "lct.post_question(question)\n",
    "print(lct.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ecc925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
